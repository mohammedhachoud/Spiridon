{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Units Test Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded everything with seed 42\n",
      "Setup complete. Modules imported.\n",
      "Using data from: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\data\n",
      "Test outputs will be in: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\test_outputs\n",
      "Testing with original ceramic IDs: [48, 49, 10601]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import project modules\n",
    "import config \n",
    "from data_loader import load_all_dataframes\n",
    "from preprocess import translate_dataframes, process_tech_cat_names\n",
    "from feature_engineering import create_ceramic_summary, generate_one_hot_embeddings\n",
    "from graph_utils import (\n",
    "    build_category_hierarchy_and_map_ceramics,\n",
    "    calculate_completeness_score,\n",
    "    extract_triplets_for_selection,\n",
    "    get_parents,\n",
    "    memo_parents\n",
    ")\n",
    "from src.data_preparation.format_rgcn_data import (\n",
    "    format_rgcn_data_with_hybrid_embeddings\n",
    ")\n",
    "# Corrected import path for adapt_rgcn_data_for_ceramic_classification\n",
    "from data_preparation.format_rgcn_mlp_classification_data import (\n",
    "    adapt_rgcn_data_for_ceramic_classification\n",
    ")\n",
    "from data_preparation.format_mlp_classification_data import (\n",
    "    create_mlp_input_data\n",
    ")\n",
    "from utils import seed_everything, get_feature_parent_relation_label\n",
    "\n",
    "# --- Configuration for the Test ---\n",
    "seed_everything(42)\n",
    "\n",
    "# LOCAL_DATA_PATH_TEST will come from the imported config module\n",
    "LOCAL_DATA_PATH_TEST = config.LOCAL_DATA_PATH\n",
    "OUTPUT_TEST_DIR = os.path.join(config.OUTPUT_BASE_DIR, \"test_outputs\")\n",
    "os.makedirs(OUTPUT_TEST_DIR, exist_ok=True)\n",
    "\n",
    "TEST_CERAMIC_IDS_ORIGINAL = [48, 49, 10601]\n",
    "\n",
    "print(\"Setup complete. Modules imported.\")\n",
    "print(f\"Using data from: {LOCAL_DATA_PATH_TEST}\") # This now uses config.LOCAL_DATA_PATH\n",
    "print(f\"Test outputs will be in: {OUTPUT_TEST_DIR}\")\n",
    "print(f\"Testing with original ceramic IDs: {TEST_CERAMIC_IDS_ORIGINAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select 3 Ceramics as Examples to treat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Raw Data for Test Ceramics from ceramic_summary_prepared.csv ---\n",
      "\n",
      "Found 3 ceramic records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_16344\\1639807925.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  ceramic_summary_df = pd.read_csv('C:/Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\ceramic_summary_prepared.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    #ceramics_table {\n",
       "        font-size: 12px;\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    #ceramics_table th, #ceramics_table td {\n",
       "        border: 1px solid #ddd;\n",
       "        padding: 8px;\n",
       "        text-align: left;\n",
       "        vertical-align: top;\n",
       "        word-wrap: break-word;\n",
       "        max-width: 300px;\n",
       "    }\n",
       "    #ceramics_table th {\n",
       "        background-color: #f2f2f2;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    #ceramics_table tr:nth-child(even) {\n",
       "        background-color: #f9f9f9;\n",
       "    }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe table table-striped table-bordered\" id=\"ceramics_table\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ceramic_id</th>\n",
       "      <th>ceramic_identifier</th>\n",
       "      <th>origin</th>\n",
       "      <th>tech_cat_id</th>\n",
       "      <th>tech_cat_name</th>\n",
       "      <th>reuse</th>\n",
       "      <th>production_fail</th>\n",
       "      <th>period_name_fr</th>\n",
       "      <th>context_type_name</th>\n",
       "      <th>identifier_origin_source_name</th>\n",
       "      <th>function_id</th>\n",
       "      <th>function_name_fr</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>feature_name_fr</th>\n",
       "      <th>color_name_list</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>19188</td>\n",
       "      <td>Sainte-Barbe, France</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown Category</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>production</td>\n",
       "      <td>LA3M/LAMM Inventory</td>\n",
       "      <td>[np.int64(11)]</td>\n",
       "      <td>['pitcher (or jug)']</td>\n",
       "      <td>[np.int64(3), np.int64(16), np.int64(41), np.int64(49)]</td>\n",
       "      <td>['Calcareous', 'Lead glaze', 'Oxidizing', 'Thrown']</td>\n",
       "      <td>['Green']</td>\n",
       "      <td>A Green ceramic with a production fault from the production in Sainte-Barbe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>26274</td>\n",
       "      <td>Collège Eugène Vigne, France</td>\n",
       "      <td>1</td>\n",
       "      <td>Category Kaolinitic from the Uzège group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>production</td>\n",
       "      <td>LA3M/LAMM Inventory</td>\n",
       "      <td>[np.int64(11)]</td>\n",
       "      <td>['pitcher (or jug)']</td>\n",
       "      <td>[np.int64(3), np.int64(19), np.int64(41), np.int64(49)]</td>\n",
       "      <td>['Kaolinitic', 'Lead glaze', 'Oxidizing', 'Thrown']</td>\n",
       "      <td>[]</td>\n",
       "      <td>A Ceramic ceramic from the production in Collège Eugène Vigne.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10601</td>\n",
       "      <td>3-8</td>\n",
       "      <td>Hôtel d'Agar, France</td>\n",
       "      <td>94</td>\n",
       "      <td>Category White kaolinitic, glazed</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>époque moderne occidentale</td>\n",
       "      <td>consumption</td>\n",
       "      <td>Site Archives</td>\n",
       "      <td>[np.int64(21)]</td>\n",
       "      <td>['cooking pot']</td>\n",
       "      <td>[np.int64(3), np.int64(19), np.int64(41), np.int64(49)]</td>\n",
       "      <td>['Kaolinitic', 'Lead glaze', 'Oxidizing', 'Thrown']</td>\n",
       "      <td>[]</td>\n",
       "      <td>A Ceramic ceramic from the consumption in Hôtel d'Agar.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Individual Ceramic Details ---\n",
      "\n",
      "==================================================\n",
      "CERAMIC ID: 48\n",
      "==================================================\n",
      "ceramic_id: 48\n",
      "------------------------------\n",
      "ceramic_identifier: 19188\n",
      "------------------------------\n",
      "origin: Sainte-Barbe, France\n",
      "------------------------------\n",
      "tech_cat_id: -1\n",
      "------------------------------\n",
      "tech_cat_name: Unknown Category\n",
      "------------------------------\n",
      "production_fail: True\n",
      "------------------------------\n",
      "context_type_name: production\n",
      "------------------------------\n",
      "identifier_origin_source_name: LA3M/LAMM Inventory\n",
      "------------------------------\n",
      "function_id: [np.int64(11)]\n",
      "------------------------------\n",
      "function_name_fr: ['pitcher (or jug)']\n",
      "------------------------------\n",
      "feature_id: [np.int64(3), np.int64(16), np.int64(41), np.int64(49)]\n",
      "------------------------------\n",
      "feature_name_fr: ['Calcareous', 'Lead glaze', 'Oxidizing', 'Thrown']\n",
      "------------------------------\n",
      "color_name_list: ['Green']\n",
      "------------------------------\n",
      "description: A Green ceramic with a production fault from the production in Sainte-Barbe.\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "CERAMIC ID: 49\n",
      "==================================================\n",
      "ceramic_id: 49\n",
      "------------------------------\n",
      "ceramic_identifier: 26274\n",
      "------------------------------\n",
      "origin: Collège Eugène Vigne, France\n",
      "------------------------------\n",
      "tech_cat_id: 1\n",
      "------------------------------\n",
      "tech_cat_name:  Category Kaolinitic from the Uzège group\n",
      "------------------------------\n",
      "production_fail: False\n",
      "------------------------------\n",
      "context_type_name: production\n",
      "------------------------------\n",
      "identifier_origin_source_name: LA3M/LAMM Inventory\n",
      "------------------------------\n",
      "function_id: [np.int64(11)]\n",
      "------------------------------\n",
      "function_name_fr: ['pitcher (or jug)']\n",
      "------------------------------\n",
      "feature_id: [np.int64(3), np.int64(19), np.int64(41), np.int64(49)]\n",
      "------------------------------\n",
      "feature_name_fr: ['Kaolinitic', 'Lead glaze', 'Oxidizing', 'Thrown']\n",
      "------------------------------\n",
      "color_name_list: []\n",
      "------------------------------\n",
      "description: A Ceramic ceramic from the production in Collège Eugène Vigne.\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      "CERAMIC ID: 10601\n",
      "==================================================\n",
      "ceramic_id: 10601\n",
      "------------------------------\n",
      "ceramic_identifier: 3-8\n",
      "------------------------------\n",
      "origin: Hôtel d'Agar, France\n",
      "------------------------------\n",
      "tech_cat_id: 94\n",
      "------------------------------\n",
      "tech_cat_name:  Category White kaolinitic, glazed\n",
      "------------------------------\n",
      "reuse: False\n",
      "------------------------------\n",
      "production_fail: False\n",
      "------------------------------\n",
      "period_name_fr: époque moderne occidentale\n",
      "------------------------------\n",
      "context_type_name: consumption\n",
      "------------------------------\n",
      "identifier_origin_source_name: Site Archives\n",
      "------------------------------\n",
      "function_id: [np.int64(21)]\n",
      "------------------------------\n",
      "function_name_fr: ['cooking pot']\n",
      "------------------------------\n",
      "feature_id: [np.int64(3), np.int64(19), np.int64(41), np.int64(49)]\n",
      "------------------------------\n",
      "feature_name_fr: ['Kaolinitic', 'Lead glaze', 'Oxidizing', 'Thrown']\n",
      "------------------------------\n",
      "color_name_list: []\n",
      "------------------------------\n",
      "description: A Ceramic ceramic from the consumption in Hôtel d'Agar.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the prepared ceramic summary data\n",
    "ceramic_summary_df = pd.read_csv('C:/Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\ceramic_summary_prepared.csv')\n",
    "\n",
    "# Set pandas display options to show full content\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display raw data for test ceramics\n",
    "print(\"--- Raw Data for Test Ceramics from ceramic_summary_prepared.csv ---\")\n",
    "\n",
    "# Filter data for all test ceramic IDs\n",
    "test_ceramics_data = ceramic_summary_df[ceramic_summary_df['ceramic_id'].isin(TEST_CERAMIC_IDS_ORIGINAL)]\n",
    "\n",
    "if not test_ceramics_data.empty:\n",
    "    print(f\"\\nFound {len(test_ceramics_data)} ceramic records\")\n",
    "    \n",
    "    # Display in HTML format for better readability\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    # Create HTML table with better formatting\n",
    "    html_table = test_ceramics_data.to_html(\n",
    "        escape=False,\n",
    "        index=False,\n",
    "        table_id='ceramics_table',\n",
    "        classes='table table-striped table-bordered'\n",
    "    )\n",
    "    \n",
    "    # Add some CSS styling for better readability\n",
    "    styled_html = f\"\"\"\n",
    "    <style>\n",
    "    #ceramics_table {{\n",
    "        font-size: 12px;\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }}\n",
    "    #ceramics_table th, #ceramics_table td {{\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "        vertical-align: top;\n",
    "        word-wrap: break-word;\n",
    "        max-width: 300px;\n",
    "    }}\n",
    "    #ceramics_table th {{\n",
    "        background-color: #f2f2f2;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    #ceramics_table tr:nth-child(even) {{\n",
    "        background-color: #f9f9f9;\n",
    "    }}\n",
    "    </style>\n",
    "    {html_table}\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(styled_html))\n",
    "    \n",
    "    # Also display each ceramic individually for detailed view\n",
    "    print(\"\\n--- Individual Ceramic Details ---\")\n",
    "    for ceramic_id in TEST_CERAMIC_IDS_ORIGINAL:\n",
    "        ceramic_row = ceramic_summary_df[ceramic_summary_df['ceramic_id'] == ceramic_id]\n",
    "        if not ceramic_row.empty:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"CERAMIC ID: {ceramic_id}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Display each column and its value\n",
    "            for column in ceramic_row.columns:\n",
    "                value = ceramic_row[column].iloc[0]\n",
    "                if pd.notna(value) and str(value).strip():\n",
    "                    print(f\"{column}: {value}\")\n",
    "                    print(\"-\" * 30)\n",
    "        else:\n",
    "            print(f\"No data found for Ceramic ID {ceramic_id}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No ceramic data found for the specified IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate One-Hot Embeddings (For mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\data\n",
      "  Loaded ceramic.csv as dfs['ceramic']\n",
      "  Loaded object_colors.csv as dfs['object_colors']\n",
      "  Loaded object_colors_attrib.csv as dfs['object_colors_attrib']\n",
      "  Loaded object_feature.csv as dfs['object_feature']\n",
      "  Loaded object_feature_combined_names.csv as dfs['object_feature_combined_names']\n",
      "  Loaded object_feature_attrib.csv as dfs['object_feature_attrib']\n",
      "  Loaded object_function_translated.csv as dfs['object_function']\n",
      "  Loaded object_function_attrib.csv as dfs['object_function_attrib']\n",
      "  Loaded tech_cat_translated.csv as dfs['tech_cat']\n",
      "  Loaded archaeological_sites.csv as dfs['archaeological_sites']\n",
      "  Loaded traditional_designation.csv as dfs['traditional_designation']\n",
      "  Loaded historical_period.csv as dfs['historical_period']\n",
      "  Loaded tech_cat_color_attrib.csv as dfs['tech_cat_color_attrib']\n",
      "  Loaded tech_cat_feature_attrib.csv as dfs['tech_cat_feature_attrib']\n",
      "  Loaded tech_cat_function_attrib.csv as dfs['tech_cat_function_attrib']\n",
      "  Loaded relations_type.csv as dfs['relations_type']\n",
      "  Loaded relations.csv as dfs['relations']\n",
      "  Loaded context_type_list.csv as dfs['context_type_list']\n",
      "  Loaded object_data_source.csv as dfs['object_data_source']\n",
      "  Loaded category_hierarchy_combined_names.csv as dfs['category_hierarchy_combined_names']\n",
      "  Loaded Features_Ontology_PF_translated.csv as dfs['Features_Ontology']\n",
      "\n",
      "--- Testing create_mlp_input_data ---\n",
      "\n",
      "--- Testing create_mlp_input_data ---\n",
      "\n",
      "--- Testing MLP Data Creation: Study 'etude1', Embedding Type 1 (Functions + Features) ---\n",
      "Preparing data for MLP Classifier...\n",
      "Study configuration: etude1\n",
      "Embedding type: 0\n",
      "Embedding strategy: Ceramic attributes only (origin, color, context, source, reuse, production_fail)\n",
      "  Root categories found (IDs): [140, 135, 144, 132, 137]\n",
      "  Total categorized ceramics before sampling: 8697\n",
      "  Original class distribution:\n",
      "    Class 132 (Categories with transparent glazes on slip): 1396 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 913 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 5301 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 949 ceramics\n",
      "  Minimum class: 140 with 138 ceramics\n",
      "  Applying etude1 strategy: 138 ceramics per class\n",
      "  Final class distribution after etude1 sampling:\n",
      "    Class 132 (Categories with transparent glazes on slip): 138 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 138 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 138 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 138 ceramics\n",
      "  Total ceramics after etude1 sampling: 690\n",
      "  Generating ceramic attribute embeddings...\n",
      "    Attribute map sizes: origin=39, color=11, context=3, source=4\n",
      "    Ceramic attribute embedding generated. Length: 61\n",
      "    Breakdown: origin(39) + color(11) + context(3) + source(4) + reuse(2) + prod_fail(2)\n",
      "  Final MLP embedding generated. Length: 61\n",
      "\n",
      "  Final MLP data shapes:\n",
      "    X shape: (690, 61)\n",
      "    y shape: (690,)\n",
      "    Unique labels: [132 135 137 140 144]\n",
      "  MLP data saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_data\\etude1_type0\n",
      "  Study configuration saved as study_config.json\n",
      "\n",
      "Data for etude1, type 1:\n",
      "  X_mlp shape: (690, 61)\n",
      "  y_mlp shape: (690,)\n",
      "  Number of unique labels: 5\n",
      "  Embedding info: {'origin_dim': 39, 'color_dim': 11, 'context_dim': 3, 'source_dim': 4, 'reuse_dim': 2, 'production_fail_dim': 2, 'total_dim': 61}\n",
      "\n",
      "--- Verifying embedding for a specific test ceramic (if present in the sample) ---\n",
      "  Example MLP Embedding (first ceramic in etude1, type 1 sample):\n",
      "    Length: 61\n",
      "    Embedding (first 20 values): [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0]\n",
      "    Corresponding label: 132 (Categories with transparent glazes on slip)\n"
     ]
    }
   ],
   "source": [
    "dfs_raw = load_all_dataframes(data_base_path=LOCAL_DATA_PATH_TEST)\n",
    "dfs_raw['ceramic_summary'] = ceramic_summary_df\n",
    "\n",
    "print(\"\\n--- Testing create_mlp_input_data ---\")\n",
    "\n",
    "dfs_for_mlp_test = dfs_raw.copy()\n",
    "\n",
    "print(\"\\n--- Testing create_mlp_input_data ---\")\n",
    "if 'ceramic_summary' not in dfs_for_mlp_test or dfs_for_mlp_test['ceramic_summary'].empty:\n",
    "    if 'ceramic_summary_df' in locals() and not ceramic_summary_df.empty:\n",
    "        dfs_for_mlp_test['ceramic_summary'] = ceramic_summary_df\n",
    "    else:\n",
    "        print(\"ERROR: ceramic_summary_df not available for MLP data creation test.\")\n",
    "\n",
    "# --- Test with Embedding Type 1 (Functions + Features) for 'etude1' sampling ---\n",
    "print(\"\\n--- Testing MLP Data Creation: Study 'etude1', Embedding Type 1 (Functions + Features) ---\")\n",
    "X_mlp_e1_t1, y_mlp_e1_t1, root_names_e1_t1, maps_e1_t1, emb_info_e1_t1 = create_mlp_input_data(\n",
    "    dfs_for_mlp_test,\n",
    "    study_name=\"etude1\",\n",
    "    embedding_type=0\n",
    ")\n",
    "\n",
    "if X_mlp_e1_t1 is not None:\n",
    "    print(f\"\\nData for etude1, type 1:\")\n",
    "    print(f\"  X_mlp shape: {X_mlp_e1_t1.shape}\")\n",
    "    print(f\"  y_mlp shape: {y_mlp_e1_t1.shape}\")\n",
    "    print(f\"  Number of unique labels: {len(np.unique(y_mlp_e1_t1))}\")\n",
    "    print(f\"  Embedding info: {emb_info_e1_t1}\")\n",
    "\n",
    "    print(\"\\n--- Verifying embedding for a specific test ceramic (if present in the sample) ---\")\n",
    "    if X_mlp_e1_t1.shape[0] > 0:\n",
    "        print(f\"  Example MLP Embedding (first ceramic in etude1, type 1 sample):\")\n",
    "        print(f\"    Length: {len(X_mlp_e1_t1[2])}\")\n",
    "        \n",
    "        print(f\"    Embedding (first 20 values): {X_mlp_e1_t1[2]}\")\n",
    "      #  print(f\"    Embedding (Functions): {X_mlp_e1_t1[2][:181]}\")\n",
    "      #  print(f\"    Embedding (Features): {X_mlp_e1_t1[2][181:]}\")\n",
    "        print(f\"    Corresponding label: {y_mlp_e1_t1[2]} ({root_names_e1_t1.get(y_mlp_e1_t1[2])})\")\n",
    "else:\n",
    "    print(\"MLP data creation for etude1, type 1 failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RGCN DATA (Bert Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A Triplets Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ceramic 48:\n",
      "  Functions: [11] (type: <class 'list'>)\n",
      "  Features: [3, 16, 41, 49] (type: <class 'list'>)\n",
      "\n",
      "Ceramic 49:\n",
      "  Functions: [11] (type: <class 'list'>)\n",
      "  Features: [3, 19, 41, 49] (type: <class 'list'>)\n",
      "\n",
      "Ceramic 10601:\n",
      "  Functions: [21] (type: <class 'list'>)\n",
      "  Features: [3, 19, 41, 49] (type: <class 'list'>)\n",
      "\n",
      "--- Triplet Extraction with Fixed Data ---\n",
      "Extracting triplets for 3 selected ceramics (handling lists)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\src\\graph_utils.py:242: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  new_col_values.append([pd.to_numeric(i, errors='ignore') for i in item])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extraction for selection. Got results structure for 3 ceramics.\n",
      "\n",
      "Triplets extracted successfully!\n",
      "\n",
      "Ceramic ID: 48\n",
      "  Categories: [-1]\n",
      "  Functions: [(11, [10, 1])]\n",
      "  Features: [('3', ['2', '1']), ('16', ['15', '14']), ('41', ['40', '35']), ('49', ['48', '47'])]\n",
      "\n",
      "Ceramic ID: 49\n",
      "  Categories: [1, 76, 135]\n",
      "  Functions: [(11, [10, 1])]\n",
      "  Features: [('3', ['2', '1']), ('19', ['15', '14']), ('41', ['40', '35']), ('49', ['48', '47'])]\n",
      "\n",
      "Ceramic ID: 10601\n",
      "  Categories: [94, 76, 135]\n",
      "  Functions: [(21, [19, 1])]\n",
      "  Features: [('3', ['2', '1']), ('19', ['15', '14']), ('41', ['40', '35']), ('49', ['48', '47'])]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to safely parse list-like strings containing numpy types\n",
    "def parse_numpy_list_string(list_str):\n",
    "    \"\"\"\n",
    "    Parse strings like '[np.int64(11)]' or '[np.int64(3), np.int64(16)]' \n",
    "    into Python lists of integers.\n",
    "    \"\"\"\n",
    "    if pd.isna(list_str) or list_str == '[]':\n",
    "        return []\n",
    "    \n",
    "    if isinstance(list_str, list):\n",
    "        return list_str  # Already a list\n",
    "    \n",
    "    if isinstance(list_str, str):\n",
    "        # Remove numpy type wrappers: np.int64(X) -> X\n",
    "        cleaned = re.sub(r'np\\.int64\\((\\d+)\\)', r'\\1', list_str)\n",
    "        \n",
    "        try:\n",
    "            # Try to evaluate as Python literal\n",
    "            parsed = ast.literal_eval(cleaned)\n",
    "            if isinstance(parsed, list):\n",
    "                return [int(x) for x in parsed]\n",
    "            else:\n",
    "                return [int(parsed)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Fallback: extract numbers using regex\n",
    "            numbers = re.findall(r'\\d+', str(list_str))\n",
    "            return [int(x) for x in numbers]\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Create a copy and fix the list columns\n",
    "ceramic_summary_fixed = ceramic_summary_df.copy()\n",
    "ceramic_summary_fixed['function_id'] = ceramic_summary_fixed['function_id'].apply(parse_numpy_list_string)\n",
    "ceramic_summary_fixed['feature_id'] = ceramic_summary_fixed['feature_id'].apply(parse_numpy_list_string)\n",
    "\n",
    "\n",
    "\n",
    "# Verify the fix worked for our test ceramics\n",
    "for ceramic_id in TEST_CERAMIC_IDS_ORIGINAL:\n",
    "    row = ceramic_summary_fixed[ceramic_summary_fixed['ceramic_id'] == ceramic_id]\n",
    "    if not row.empty:\n",
    "        print(f\"\\nCeramic {ceramic_id}:\")\n",
    "        print(f\"  Functions: {row['function_id'].iloc[0]} (type: {type(row['function_id'].iloc[0])})\")\n",
    "        print(f\"  Features: {row['feature_id'].iloc[0]} (type: {type(row['feature_id'].iloc[0])})\")\n",
    "\n",
    "# Now retry the triplet extraction with the fixed data\n",
    "print(\"\\n--- Triplet Extraction with Fixed Data ---\")\n",
    "dfs_for_triplets_fixed = {\n",
    "    'ceramic_summary': ceramic_summary_fixed,  # Use the fixed version\n",
    "    'tech_cat': dfs_raw['tech_cat'],\n",
    "    'object_function': dfs_raw['object_function'],\n",
    "    'Features_Ontology': dfs_raw['Features_Ontology']\n",
    "}\n",
    "\n",
    "triplets_for_test_ceramics = extract_triplets_for_selection(TEST_CERAMIC_IDS_ORIGINAL, dfs_for_triplets_fixed)\n",
    "\n",
    "if triplets_for_test_ceramics:\n",
    "    print(\"\\nTriplets extracted successfully!\")\n",
    "    for entry in triplets_for_test_ceramics:\n",
    "        if entry['ceramic_id'] in TEST_CERAMIC_IDS_ORIGINAL:\n",
    "            print(f\"\\nCeramic ID: {entry['ceramic_id']}\")\n",
    "            print(f\"  Categories: {[cat['category_id'] for cat in entry['categories']]}\")\n",
    "            print(f\"  Functions: {entry['functions']}\")\n",
    "            print(f\"  Features: {entry['features']}\")\n",
    "else:\n",
    "    print(\"  Triplet extraction still failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Embedding Generation and Relations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Formatting RGCN Data for Link Prediction (Test Sample with BERT for Ceramics) ---\n",
      "  Formatting test_study_link_pred data for RGCN with ALL-BERT Embeddings (BERT: paraphrase-multilingual-mpnet-base-v2)...\n",
      "    Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loaded Sentence-BERT model 'paraphrase-multilingual-mpnet-base-v2' (Native Dim: 768)\n",
      "    Building category hierarchy to get root mapping (using original category IDs)...\n",
      "    Obtained mapping for 229 original categories to their original root IDs.\n",
      "    Identifying all unique nodes in the sampled data (Ceramics, Functions, Features, and ONLY ROOT Categories)...\n",
      "    Found 26 unique node identifiers in the sample (ceramics, functions, features, and ROOT categories).\n",
      "    Assigning graph indices and generating ALL-BERT embeddings...\n",
      "    ALL-BERT Embeddings - Generated: 26, Failed/Missing Text: 0\n",
      "\n",
      "    --- Node Type Examples with BERT Embeddings ---\n",
      "    Ceramic Node Example:\n",
      "      Node ID: Ceramic_10601\n",
      "      Text: 'A Ceramic ceramic from the consumption in Hôtel d'Agar.'\n",
      "      Embedding (first 5 dims): [ 0.10687211 -0.12885433 -0.01231622  0.12119691 -0.04013303]\n",
      "\n",
      "    Function Node Example:\n",
      "      Node ID: Func_1\n",
      "      Text: 'dishware (or tableware)'\n",
      "      Embedding (first 5 dims): [ 0.0166124  -0.11477309 -0.00935472 -0.12030761 -0.0673581 ]\n",
      "\n",
      "    Category Node Example:\n",
      "      Node ID: Cat_132_Root_132\n",
      "      Text: 'Categories with transparent glazes on slip'\n",
      "      Embedding (first 5 dims): [ 0.04153645 -0.07711426 -0.01183737  0.10898539  0.06632965]\n",
      "\n",
      "    Feature Node Example:\n",
      "      Node ID: Feat_1\n",
      "      Text: 'Forming/Shaping'\n",
      "      Embedding (first 5 dims): [-0.15521666 -0.25642768 -0.01476311  0.05541804  0.01365412]\n",
      "\n",
      "    --- End Node Type Examples ---\n",
      "\n",
      "    Populated final ALL-BERT embedding matrix with shape (26 nodes, 768 dim). 26 nodes have non-zero embeddings.\n",
      "    Identifying root category node indices in the final graph map (all 'Cat_' nodes are roots)...\n",
      "    Identified 5 unique root category nodes in the graph's node_to_idx map.\n",
      "    Created cat_idx_to_root_idx_map with 5 entries (each root maps to itself).\n",
      "    Processing triplets using pre-assigned graph indices...\n",
      "    Added 0 RootCat->Function and 1 RootCat->Feature triplets (direct connections only).\n",
      "    Removed 17 duplicate training triplets.\n",
      "    Removed 0 duplicate evaluation triplets (Ceramic->RootCat).\n",
      "\n",
      "    --- RGCN Data Summary (Hybrid Embeddings with PCA) ---\n",
      "    Study: test_study_link_pred\n",
      "    BERT Model for Non-Ceramics: paraphrase-multilingual-mpnet-base-v2 (Native Dim: 768)\n",
      "    Ceramic Embeddings Source: OHE from 'Embedding' column (Expected Dim: 768)\n",
      "    Final Hybrid Embedding Dimension: 768\n",
      "    Total Nodes in Graph: 26\n",
      "    Nodes with Non-Zero Embedding in Final Matrix: 26\n",
      "    Total Relation Types in Graph: 7\n",
      "    Training Triplets (for GNN structure): 29\n",
      "    Evaluation Triplets (Ceramic->RootCat, for BTC task): 2\n",
      "    Number of Root Category Nodes in Graph (for neg. sampling): 5\n",
      "    --- End Summary ---\n",
      "\n",
      "RGCN Data Formatted (sample stats):\n",
      "  Num Nodes: 26\n",
      "  Num Relations: 7\n",
      "  Embedding Dim: 768\n",
      "  Training Triplets: 29\n",
      "  Evaluation Triplets (Ceramic->RootCat): 2\n",
      "  Embedding for Ceramic_48 (idx 6, first 5 values): [-0.0198269  -0.13833101 -0.01217625  0.14767088 -0.03060706]\n",
      "  Is embedding for Ceramic_48 all zeros? False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Formatting RGCN Data for Link Prediction (Test Sample with BERT for Ceramics) ---\")\n",
    "dfs_for_rgcn_format = dfs_raw.copy() \n",
    "dfs_for_rgcn_format['ceramic_summary'] = ceramic_summary_df \n",
    "\n",
    "if triplets_for_test_ceramics:\n",
    "    rgcn_data_test = format_rgcn_data_with_hybrid_embeddings(\n",
    "        dfs=dfs_for_rgcn_format,\n",
    "        triplets_for_study=triplets_for_test_ceramics, \n",
    "        study_name=\"test_study_link_pred\"\n",
    "    )\n",
    "\n",
    "    if rgcn_data_test:\n",
    "        print(\"\\nRGCN Data Formatted (sample stats):\")\n",
    "        print(f\"  Num Nodes: {rgcn_data_test['num_nodes']}\")\n",
    "        print(f\"  Num Relations: {rgcn_data_test['num_relations']}\")\n",
    "        print(f\"  Embedding Dim: {rgcn_data_test['embedding_dim']}\")\n",
    "        print(f\"  Training Triplets: {len(rgcn_data_test['training_triplets'])}\")\n",
    "        print(f\"  Evaluation Triplets (Ceramic->RootCat): {len(rgcn_data_test['evaluation_triplets'])}\")\n",
    "        \n",
    "        # Check embeddings for one of our test ceramics\n",
    "        test_ceramic_graph_id_str = f\"Ceramic_{TEST_CERAMIC_IDS_ORIGINAL[0]}\"\n",
    "        if test_ceramic_graph_id_str in rgcn_data_test['node_to_idx']:\n",
    "            idx = rgcn_data_test['node_to_idx'][test_ceramic_graph_id_str]\n",
    "            print(f\"  Embedding for {test_ceramic_graph_id_str} (idx {idx}, first 5 values): {rgcn_data_test['node_embeddings'][idx][:5]}\")\n",
    "            is_zero = np.all(rgcn_data_test['node_embeddings'][idx] == 0)\n",
    "            print(f\"  Is embedding for {test_ceramic_graph_id_str} all zeros? {is_zero}\")\n",
    "    else:\n",
    "        print(\"  RGCN data formatting failed.\")\n",
    "else:\n",
    "    print(\"  Skipping RGCN data formatting as triplet extraction failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C Reformat For Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Adapting RGCN Data for Classification (Test Sample) ---\n",
      "\n",
      "--- Adapting RGCN Data for Ceramic Classification (Root Labels) ---\n",
      "  Original nodes: 26\n",
      "  Nodes kept for classification: 26\n",
      "  Extracting root category labels from 31 available triplets...\n",
      "  Looking for relation ID 0 ('BELONGS_TO_CATEGORY')\n",
      "  Extracted labels for 2 ceramic nodes. Found 1 unique root labels.\n",
      "  Label mapping (1 labels):\n",
      "    Label 0: 'Categories with transparent glazes' (Original Root ID: 135, 2 ceramics)\n",
      "  Identified 3 ceramic nodes in the new mapping for classification.\n",
      "\n",
      "--- Adaptation for Classification Complete ---\n",
      "  Final ceramic nodes with labels: 2\n",
      "  Total classes: 1\n",
      "\n",
      "Classification Data Adapted (sample stats):\n",
      "  Num Nodes (for classifier graph): 26\n",
      "  Num Classes: 1\n",
      "  Ceramic Labels (first 5): {5: 0, 7: 0}\n",
      "  Label to Category Name: {0: 'Categories with transparent glazes'}\n",
      "  Ceramic_48 (new_idx 6) not found in ceramic_labels.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Adapting RGCN Data for Classification (Test Sample) ---\")\n",
    "if rgcn_data_test: \n",
    "    classification_data_test = adapt_rgcn_data_for_ceramic_classification(rgcn_data_test, dfs_raw)\n",
    "    \n",
    "    if classification_data_test:\n",
    "        print(\"\\nClassification Data Adapted (sample stats):\")\n",
    "        print(f\"  Num Nodes (for classifier graph): {classification_data_test['num_nodes']}\")\n",
    "        print(f\"  Num Classes: {classification_data_test['stats']['classification_num_classes']}\")\n",
    "        print(f\"  Ceramic Labels (first 5): {dict(list(classification_data_test['ceramic_labels'].items())[:5])}\")\n",
    "        print(f\"  Label to Category Name: {classification_data_test['label_to_category_name']}\")\n",
    "\n",
    "        original_ceramic_id_for_label_check = TEST_CERAMIC_IDS_ORIGINAL[0]\n",
    "        ceramic_identifier_for_label_check = f\"Ceramic_{original_ceramic_id_for_label_check}\"\n",
    "        \n",
    "        if ceramic_identifier_for_label_check in classification_data_test['node_to_idx']:\n",
    "            new_ceramic_idx_clf = classification_data_test['node_to_idx'][ceramic_identifier_for_label_check]\n",
    "            if new_ceramic_idx_clf in classification_data_test['ceramic_labels']:\n",
    "                label_id = classification_data_test['ceramic_labels'][new_ceramic_idx_clf]\n",
    "                label_name = classification_data_test['label_to_category_name'].get(label_id)\n",
    "                print(f\"  Label for {ceramic_identifier_for_label_check} (new_idx {new_ceramic_idx_clf}): Label ID {label_id} ({label_name})\")\n",
    "            else:\n",
    "                print(f\"  {ceramic_identifier_for_label_check} (new_idx {new_ceramic_idx_clf}) not found in ceramic_labels.\")\n",
    "        else:\n",
    "            print(f\"  {ceramic_identifier_for_label_check} not found in node_to_idx of classification data.\")\n",
    "    else:\n",
    "        print(\"  Classification data adaptation failed.\")\n",
    "else:\n",
    "    print(\"  Skipping classification data adaptation as RGCN data formatting failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MLP\n",
    "#### 4.1.1 MLP with Embedding  Type 0: Ceramic attributes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing MLP Training and Evaluation Pipeline ---\n",
      "NOTE: This test uses the full data loaded in previous steps to train the model.\n",
      "\n",
      "Running MLP classification for study: 'etude1' with embedding type: 0\n",
      "\n",
      "============================================================\n",
      "STARTING MLP CLASSIFICATION SCENARIOS FOR: etude1\n",
      "============================================================\n",
      "\n",
      "Preparing MLP input data with multi-label encoding...\n",
      "Preparing data for MLP Classifier...\n",
      "Study configuration: etude1\n",
      "Embedding type: 0\n",
      "Embedding strategy: Ceramic attributes only (origin, color, context, source, reuse, production_fail)\n",
      "  Root categories found (IDs): [140, 135, 144, 132, 137]\n",
      "  Total categorized ceramics before sampling: 8697\n",
      "  Original class distribution:\n",
      "    Class 132 (Categories with transparent glazes on slip): 1396 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 913 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 5301 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 949 ceramics\n",
      "  Minimum class: 140 with 138 ceramics\n",
      "  Applying etude1 strategy: 138 ceramics per class\n",
      "  Final class distribution after etude1 sampling:\n",
      "    Class 132 (Categories with transparent glazes on slip): 138 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 138 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 138 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 138 ceramics\n",
      "  Total ceramics after etude1 sampling: 690\n",
      "  Generating ceramic attribute embeddings...\n",
      "    Attribute map sizes: origin=39, color=11, context=3, source=4\n",
      "    Ceramic attribute embedding generated. Length: 61\n",
      "    Breakdown: origin(39) + color(11) + context(3) + source(4) + reuse(2) + prod_fail(2)\n",
      "  Final MLP embedding generated. Length: 61\n",
      "\n",
      "  Final MLP data shapes:\n",
      "    X shape: (690, 61)\n",
      "    y shape: (690,)\n",
      "    Unique labels: [132 135 137 140 144]\n",
      "  MLP data saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_data\\etude1_type0\n",
      "  Study configuration saved as study_config.json\n",
      "Data prepared successfully:\n",
      "  X shape: (690, 61)\n",
      "  y shape: (690,)\n",
      "  Unique classes: 5\n",
      "Train/Test split completed:\n",
      "  Training samples: 586\n",
      "  Test samples: 104\n",
      "\n",
      "==================================================\n",
      "RUNNING SCENARIO: baseline_architecture\n",
      "Architecture: (256, 128)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "SCENARIO: etude1_baseline_architecture - MLP Classifier\n",
      "==================================================\n",
      "Training samples: 586\n",
      "Test samples: 104\n",
      "Input features: 61\n",
      "Number of classes (train): 5\n",
      "Number of classes (test): 5\n",
      "Training distribution: Counter({np.int64(137): 118, np.int64(132): 117, np.int64(140): 117, np.int64(135): 117, np.int64(144): 117})\n",
      "Test distribution: Counter({np.int64(144): 21, np.int64(132): 21, np.int64(135): 21, np.int64(140): 21, np.int64(137): 20})\n",
      "\n",
      "Neural Network Architecture:\n",
      "Input layer: 61 features\n",
      "Hidden layers: (256, 128)\n",
      "Output layer: 5 neurons (softmax)\n",
      "\n",
      "Training MLP model...\n",
      "\n",
      "==============================\n",
      "TRAINING METRICS:\n",
      "==============================\n",
      "Training accuracy: 0.8191\n",
      "Test accuracy: 0.7692\n",
      "\n",
      "==============================\n",
      "DETAILED CLASSIFICATION REPORT:\n",
      "==============================\n",
      "                                                                      precision    recall  f1-score   support\n",
      "\n",
      "                          Categories with transparent glazes on slip       0.69      0.52      0.59        21\n",
      "                                  Categories with transparent glazes       0.67      0.76      0.71        21\n",
      "Unglazed categories (literally: Categories without vitreous coating)       1.00      0.75      0.86        20\n",
      "                                     Categories in artificial pastes       0.80      0.95      0.87        21\n",
      "                         Categories with opaque or opacified coating       0.75      0.86      0.80        21\n",
      "\n",
      "                                                            accuracy                           0.77       104\n",
      "                                                           macro avg       0.78      0.77      0.77       104\n",
      "                                                        weighted avg       0.78      0.77      0.77       104\n",
      "\n",
      "\n",
      "==============================\n",
      "PERFORMANCE BY ROOT CATEGORY:\n",
      "==============================\n",
      "Categories with transparent glazes on slip (ID: 132): 0.5238 accuracy (21 samples)\n",
      "Categories with transparent glazes (ID: 135): 0.7619 accuracy (21 samples)\n",
      "Unglazed categories (literally: Categories without vitreous coating) (ID: 137): 0.7500 accuracy (20 samples)\n",
      "Categories in artificial pastes (ID: 140): 0.9524 accuracy (21 samples)\n",
      "Categories with opaque or opacified coating (ID: 144): 0.8571 accuracy (21 samples)\n",
      "\n",
      "==============================\n",
      "CONFUSION MATRIX:\n",
      "==============================\n",
      "Rows: True labels, Columns: Predicted labels\n",
      "                                                                      Categories with transparent glazes on slip  \\\n",
      "Categories with transparent glazes on slip                                                                    11   \n",
      "Categories with transparent glazes                                                                             3   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                           0   \n",
      "Categories in artificial pastes                                                                                1   \n",
      "Categories with opaque or opacified coating                                                                    1   \n",
      "\n",
      "                                                                      Categories with transparent glazes  \\\n",
      "Categories with transparent glazes on slip                                                             5   \n",
      "Categories with transparent glazes                                                                    16   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                   1   \n",
      "Categories in artificial pastes                                                                        0   \n",
      "Categories with opaque or opacified coating                                                            2   \n",
      "\n",
      "                                                                      Unglazed categories (literally: Categories without vitreous coating)  \\\n",
      "Categories with transparent glazes on slip                                                                                               0   \n",
      "Categories with transparent glazes                                                                                                       0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                                                    15   \n",
      "Categories in artificial pastes                                                                                                          0   \n",
      "Categories with opaque or opacified coating                                                                                              0   \n",
      "\n",
      "                                                                      Categories in artificial pastes  \\\n",
      "Categories with transparent glazes on slip                                                          1   \n",
      "Categories with transparent glazes                                                                  0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                4   \n",
      "Categories in artificial pastes                                                                    20   \n",
      "Categories with opaque or opacified coating                                                         0   \n",
      "\n",
      "                                                                      Categories with opaque or opacified coating  \n",
      "Categories with transparent glazes on slip                                                                      4  \n",
      "Categories with transparent glazes                                                                              2  \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                            0  \n",
      "Categories in artificial pastes                                                                                 0  \n",
      "Categories with opaque or opacified coating                                                                    18  \n",
      "Generating plots for baseline_architecture...\n",
      "  Saved MLP training history plot to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\\etude1_baseline_architecture_training_history.png\n",
      "Model and results saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\n",
      "Test Accuracy for baseline_architecture: 0.7692\n",
      "\n",
      "--- MLP Training Test Complete ---\n",
      "Summary of results:\n",
      "  ✓ Scenario 'baseline_architecture' | Test Accuracy: 0.7692\n",
      "\n",
      "Models, scalers, and metadata saved in subdirectories of: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\n"
     ]
    }
   ],
   "source": [
    "# Import the main function for MLP classification\n",
    "from src.main_mlp_classification import run_mlp_classification_scenarios\n",
    "\n",
    "print(\"\\n--- Testing MLP Training and Evaluation Pipeline ---\")\n",
    "print(\"NOTE: This test uses the full data loaded in previous steps to train the model.\")\n",
    "\n",
    "STUDY_NAME_MLP = \"etude1\"\n",
    "# Type 0: Ceramic attributes only | Type 1: Functions/Features only | Type 2: Combined\n",
    "EMBEDDING_TYPE_MLP = 0\n",
    "\n",
    "print(f\"\\nRunning MLP classification for study: '{STUDY_NAME_MLP}' with embedding type: {EMBEDDING_TYPE_MLP}\")\n",
    "\n",
    "mlp_test_results = run_mlp_classification_scenarios(\n",
    "    dfs=dfs_raw,\n",
    "    study_name=STUDY_NAME_MLP,\n",
    "    embedding_type=EMBEDDING_TYPE_MLP\n",
    ")\n",
    "\n",
    "print(\"\\n--- MLP Training Test Complete ---\")\n",
    "if mlp_test_results:\n",
    "    print(\"Summary of results:\")\n",
    "    for scenario, accuracy in mlp_test_results.items():\n",
    "        if accuracy is not None:\n",
    "            print(f\"  ✓ Scenario '{scenario}' | Test Accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Scenario '{scenario}' failed.\")\n",
    "    output_dir = os.path.join(config.OUTPUT_BASE_DIR, 'mlp_classification_results', STUDY_NAME_MLP)\n",
    "    print(f\"\\nModels, scalers, and metadata saved in subdirectories of: {output_dir}\")\n",
    "else:\n",
    "    print(\"MLP training scenarios did not produce any results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 MLP with Embedding Type 1: Functions/Features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing MLP Training and Evaluation Pipeline ---\n",
      "NOTE: This test uses the full data loaded in previous steps to train the model.\n",
      "\n",
      "Running MLP classification for study: 'etude1' with embedding type: 1\n",
      "\n",
      "============================================================\n",
      "STARTING MLP CLASSIFICATION SCENARIOS FOR: etude1\n",
      "============================================================\n",
      "\n",
      "Preparing MLP input data with multi-label encoding...\n",
      "Preparing data for MLP Classifier...\n",
      "Study configuration: etude1\n",
      "Embedding type: 1\n",
      "Embedding strategy: Functions + Features multi-label one-hot encoding\n",
      "  Root categories found (IDs): [140, 135, 144, 132, 137]\n",
      "  Total categorized ceramics before sampling: 8697\n",
      "  Original class distribution:\n",
      "    Class 132 (Categories with transparent glazes on slip): 1396 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 913 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 5301 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 949 ceramics\n",
      "  Minimum class: 140 with 138 ceramics\n",
      "  Applying etude1 strategy: 138 ceramics per class\n",
      "  Final class distribution after etude1 sampling:\n",
      "    Class 132 (Categories with transparent glazes on slip): 138 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 138 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 138 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 138 ceramics\n",
      "  Total ceramics after etude1 sampling: 690\n",
      "  Generating function/feature embeddings...\n",
      "    DataFrame shape: (690, 17)\n",
      "    Sample function_id: '[np.int64(5)]'\n",
      "    Sample feature_id: '[np.int64(3), np.int64(16), np.int64(41), np.int64(46), np.int64(49)]'\n",
      "    Function embedding length: 181\n",
      "    Available function IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21)]...\n",
      "    Feature embedding length: 105\n",
      "    Available feature IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21)]...\n",
      "    Testing on first 3 rows...\n",
      "      Row 0 function_id parsed: [5]\n",
      "      Row 0 feature_id parsed: [3, 16, 41, 46, 49]\n",
      "      Row 0 embedding sum: 6\n",
      "      Row 1 function_id parsed: [2]\n",
      "      Row 1 feature_id parsed: [3, 18, 41, 46, 49, 59]\n",
      "      Row 1 embedding sum: 7\n",
      "      Row 2 function_id parsed: [3]\n",
      "      Row 2 feature_id parsed: [3, 16, 41, 46, 49]\n",
      "      Row 2 embedding sum: 6\n",
      "    Applying to all rows...\n",
      "    Function/feature embedding generated. Length: 286\n",
      "    Breakdown: functions(181) + features(105)\n",
      "    Embeddings with non-zero values: 690/690\n",
      "    SUCCESS: 690 rows have valid embeddings!\n",
      "  Final MLP embedding generated. Length: 286\n",
      "\n",
      "  Final MLP data shapes:\n",
      "    X shape: (690, 286)\n",
      "    y shape: (690,)\n",
      "    Unique labels: [132 135 137 140 144]\n",
      "  MLP data saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_data\\etude1_type1\n",
      "  Study configuration saved as study_config.json\n",
      "Data prepared successfully:\n",
      "  X shape: (690, 286)\n",
      "  y shape: (690,)\n",
      "  Unique classes: 5\n",
      "Train/Test split completed:\n",
      "  Training samples: 586\n",
      "  Test samples: 104\n",
      "\n",
      "==================================================\n",
      "RUNNING SCENARIO: baseline_architecture\n",
      "Architecture: (256, 128)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "SCENARIO: etude1_baseline_architecture - MLP Classifier\n",
      "==================================================\n",
      "Training samples: 586\n",
      "Test samples: 104\n",
      "Input features: 286\n",
      "Number of classes (train): 5\n",
      "Number of classes (test): 5\n",
      "Training distribution: Counter({np.int64(137): 118, np.int64(132): 117, np.int64(140): 117, np.int64(135): 117, np.int64(144): 117})\n",
      "Test distribution: Counter({np.int64(144): 21, np.int64(132): 21, np.int64(135): 21, np.int64(140): 21, np.int64(137): 20})\n",
      "\n",
      "Neural Network Architecture:\n",
      "Input layer: 286 features\n",
      "Hidden layers: (256, 128)\n",
      "Output layer: 5 neurons (softmax)\n",
      "\n",
      "Training MLP model...\n",
      "\n",
      "==============================\n",
      "TRAINING METRICS:\n",
      "==============================\n",
      "Training accuracy: 0.9727\n",
      "Test accuracy: 0.9327\n",
      "\n",
      "==============================\n",
      "DETAILED CLASSIFICATION REPORT:\n",
      "==============================\n",
      "                                                                      precision    recall  f1-score   support\n",
      "\n",
      "                          Categories with transparent glazes on slip       1.00      0.76      0.86        21\n",
      "                                  Categories with transparent glazes       0.84      1.00      0.91        21\n",
      "Unglazed categories (literally: Categories without vitreous coating)       1.00      0.90      0.95        20\n",
      "                                     Categories in artificial pastes       1.00      1.00      1.00        21\n",
      "                         Categories with opaque or opacified coating       0.88      1.00      0.93        21\n",
      "\n",
      "                                                            accuracy                           0.93       104\n",
      "                                                           macro avg       0.94      0.93      0.93       104\n",
      "                                                        weighted avg       0.94      0.93      0.93       104\n",
      "\n",
      "\n",
      "==============================\n",
      "PERFORMANCE BY ROOT CATEGORY:\n",
      "==============================\n",
      "Categories with transparent glazes on slip (ID: 132): 0.7619 accuracy (21 samples)\n",
      "Categories with transparent glazes (ID: 135): 1.0000 accuracy (21 samples)\n",
      "Unglazed categories (literally: Categories without vitreous coating) (ID: 137): 0.9000 accuracy (20 samples)\n",
      "Categories in artificial pastes (ID: 140): 1.0000 accuracy (21 samples)\n",
      "Categories with opaque or opacified coating (ID: 144): 1.0000 accuracy (21 samples)\n",
      "\n",
      "==============================\n",
      "CONFUSION MATRIX:\n",
      "==============================\n",
      "Rows: True labels, Columns: Predicted labels\n",
      "                                                                      Categories with transparent glazes on slip  \\\n",
      "Categories with transparent glazes on slip                                                                    16   \n",
      "Categories with transparent glazes                                                                             0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                           0   \n",
      "Categories in artificial pastes                                                                                0   \n",
      "Categories with opaque or opacified coating                                                                    0   \n",
      "\n",
      "                                                                      Categories with transparent glazes  \\\n",
      "Categories with transparent glazes on slip                                                             3   \n",
      "Categories with transparent glazes                                                                    21   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                   1   \n",
      "Categories in artificial pastes                                                                        0   \n",
      "Categories with opaque or opacified coating                                                            0   \n",
      "\n",
      "                                                                      Unglazed categories (literally: Categories without vitreous coating)  \\\n",
      "Categories with transparent glazes on slip                                                                                               0   \n",
      "Categories with transparent glazes                                                                                                       0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                                                    18   \n",
      "Categories in artificial pastes                                                                                                          0   \n",
      "Categories with opaque or opacified coating                                                                                              0   \n",
      "\n",
      "                                                                      Categories in artificial pastes  \\\n",
      "Categories with transparent glazes on slip                                                          0   \n",
      "Categories with transparent glazes                                                                  0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                0   \n",
      "Categories in artificial pastes                                                                    21   \n",
      "Categories with opaque or opacified coating                                                         0   \n",
      "\n",
      "                                                                      Categories with opaque or opacified coating  \n",
      "Categories with transparent glazes on slip                                                                      2  \n",
      "Categories with transparent glazes                                                                              0  \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                            1  \n",
      "Categories in artificial pastes                                                                                 0  \n",
      "Categories with opaque or opacified coating                                                                    21  \n",
      "Generating plots for baseline_architecture...\n",
      "  Saved MLP training history plot to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\\etude1_baseline_architecture_training_history.png\n",
      "Model and results saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\n",
      "Test Accuracy for baseline_architecture: 0.9327\n",
      "\n",
      "--- MLP Training Test Complete ---\n",
      "Summary of results:\n",
      "  ✓ Scenario 'baseline_architecture' | Test Accuracy: 0.9327\n",
      "\n",
      "Models, scalers, and metadata saved in subdirectories of: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\n"
     ]
    }
   ],
   "source": [
    "# Import the main function for MLP classification\n",
    "from src.main_mlp_classification import run_mlp_classification_scenarios\n",
    "\n",
    "print(\"\\n--- Testing MLP Training and Evaluation Pipeline ---\")\n",
    "print(\"NOTE: This test uses the full data loaded in previous steps to train the model.\")\n",
    "\n",
    "STUDY_NAME_MLP = \"etude1\"\n",
    "# Type 0: Ceramic attributes only | Type 1: Functions/Features only | Type 2: Combined\n",
    "EMBEDDING_TYPE_MLP = 1\n",
    "\n",
    "print(f\"\\nRunning MLP classification for study: '{STUDY_NAME_MLP}' with embedding type: {EMBEDDING_TYPE_MLP}\")\n",
    "\n",
    "mlp_test_results = run_mlp_classification_scenarios(\n",
    "    dfs=dfs_raw,\n",
    "    study_name=STUDY_NAME_MLP,\n",
    "    embedding_type=EMBEDDING_TYPE_MLP\n",
    ")\n",
    "\n",
    "print(\"\\n--- MLP Training Test Complete ---\")\n",
    "if mlp_test_results:\n",
    "    print(\"Summary of results:\")\n",
    "    for scenario, accuracy in mlp_test_results.items():\n",
    "        if accuracy is not None:\n",
    "            print(f\"  ✓ Scenario '{scenario}' | Test Accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Scenario '{scenario}' failed.\")\n",
    "    output_dir = os.path.join(config.OUTPUT_BASE_DIR, 'mlp_classification_results', STUDY_NAME_MLP)\n",
    "    print(f\"\\nModels, scalers, and metadata saved in subdirectories of: {output_dir}\")\n",
    "else:\n",
    "    print(\"MLP training scenarios did not produce any results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 MLP with Embedding  Type 2: Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing MLP Training and Evaluation Pipeline ---\n",
      "NOTE: This test uses the full data loaded in previous steps to train the model.\n",
      "\n",
      "Running MLP classification for study: 'etude1' with embedding type: 2\n",
      "\n",
      "============================================================\n",
      "STARTING MLP CLASSIFICATION SCENARIOS FOR: etude1\n",
      "============================================================\n",
      "\n",
      "Preparing MLP input data with multi-label encoding...\n",
      "Preparing data for MLP Classifier...\n",
      "Study configuration: etude1\n",
      "Embedding type: 2\n",
      "Embedding strategy: Combined: Ceramic attributes + Functions + Features\n",
      "  Root categories found (IDs): [140, 135, 144, 132, 137]\n",
      "  Total categorized ceramics before sampling: 8697\n",
      "  Original class distribution:\n",
      "    Class 132 (Categories with transparent glazes on slip): 1396 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 913 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 5301 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 949 ceramics\n",
      "  Minimum class: 140 with 138 ceramics\n",
      "  Applying etude1 strategy: 138 ceramics per class\n",
      "  Final class distribution after etude1 sampling:\n",
      "    Class 132 (Categories with transparent glazes on slip): 138 ceramics\n",
      "    Class 135 (Categories with transparent glazes): 138 ceramics\n",
      "    Class 137 (Unglazed categories (literally: Categories without vitreous coating)): 138 ceramics\n",
      "    Class 140 (Categories in artificial pastes): 138 ceramics\n",
      "    Class 144 (Categories with opaque or opacified coating): 138 ceramics\n",
      "  Total ceramics after etude1 sampling: 690\n",
      "  Generating ceramic attribute embeddings...\n",
      "    Attribute map sizes: origin=39, color=11, context=3, source=4\n",
      "    Ceramic attribute embedding generated. Length: 61\n",
      "    Breakdown: origin(39) + color(11) + context(3) + source(4) + reuse(2) + prod_fail(2)\n",
      "  Generating function/feature embeddings...\n",
      "    DataFrame shape: (690, 18)\n",
      "    Sample function_id: '[np.int64(5)]'\n",
      "    Sample feature_id: '[np.int64(3), np.int64(16), np.int64(41), np.int64(46), np.int64(49)]'\n",
      "    Function embedding length: 181\n",
      "    Available function IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21)]...\n",
      "    Feature embedding length: 105\n",
      "    Available feature IDs: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21)]...\n",
      "    Testing on first 3 rows...\n",
      "      Row 0 function_id parsed: [5]\n",
      "      Row 0 feature_id parsed: [3, 16, 41, 46, 49]\n",
      "      Row 0 embedding sum: 6\n",
      "      Row 1 function_id parsed: [2]\n",
      "      Row 1 feature_id parsed: [3, 18, 41, 46, 49, 59]\n",
      "      Row 1 embedding sum: 7\n",
      "      Row 2 function_id parsed: [3]\n",
      "      Row 2 feature_id parsed: [3, 16, 41, 46, 49]\n",
      "      Row 2 embedding sum: 6\n",
      "    Applying to all rows...\n",
      "    Function/feature embedding generated. Length: 286\n",
      "    Breakdown: functions(181) + features(105)\n",
      "    Embeddings with non-zero values: 690/690\n",
      "    SUCCESS: 690 rows have valid embeddings!\n",
      "  Final MLP embedding generated. Length: 347\n",
      "\n",
      "  Final MLP data shapes:\n",
      "    X shape: (690, 347)\n",
      "    y shape: (690,)\n",
      "    Unique labels: [132 135 137 140 144]\n",
      "  MLP data saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_data\\etude1_type2\n",
      "  Study configuration saved as study_config.json\n",
      "Data prepared successfully:\n",
      "  X shape: (690, 347)\n",
      "  y shape: (690,)\n",
      "  Unique classes: 5\n",
      "Train/Test split completed:\n",
      "  Training samples: 586\n",
      "  Test samples: 104\n",
      "\n",
      "==================================================\n",
      "RUNNING SCENARIO: baseline_architecture\n",
      "Architecture: (256, 128)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "SCENARIO: etude1_baseline_architecture - MLP Classifier\n",
      "==================================================\n",
      "Training samples: 586\n",
      "Test samples: 104\n",
      "Input features: 347\n",
      "Number of classes (train): 5\n",
      "Number of classes (test): 5\n",
      "Training distribution: Counter({np.int64(137): 118, np.int64(132): 117, np.int64(140): 117, np.int64(135): 117, np.int64(144): 117})\n",
      "Test distribution: Counter({np.int64(144): 21, np.int64(132): 21, np.int64(135): 21, np.int64(140): 21, np.int64(137): 20})\n",
      "\n",
      "Neural Network Architecture:\n",
      "Input layer: 347 features\n",
      "Hidden layers: (256, 128)\n",
      "Output layer: 5 neurons (softmax)\n",
      "\n",
      "Training MLP model...\n",
      "\n",
      "==============================\n",
      "TRAINING METRICS:\n",
      "==============================\n",
      "Training accuracy: 0.9863\n",
      "Test accuracy: 0.9423\n",
      "\n",
      "==============================\n",
      "DETAILED CLASSIFICATION REPORT:\n",
      "==============================\n",
      "                                                                      precision    recall  f1-score   support\n",
      "\n",
      "                          Categories with transparent glazes on slip       1.00      0.81      0.89        21\n",
      "                                  Categories with transparent glazes       0.81      1.00      0.89        21\n",
      "Unglazed categories (literally: Categories without vitreous coating)       1.00      0.95      0.97        20\n",
      "                                     Categories in artificial pastes       1.00      1.00      1.00        21\n",
      "                         Categories with opaque or opacified coating       0.95      0.95      0.95        21\n",
      "\n",
      "                                                            accuracy                           0.94       104\n",
      "                                                           macro avg       0.95      0.94      0.94       104\n",
      "                                                        weighted avg       0.95      0.94      0.94       104\n",
      "\n",
      "\n",
      "==============================\n",
      "PERFORMANCE BY ROOT CATEGORY:\n",
      "==============================\n",
      "Categories with transparent glazes on slip (ID: 132): 0.8095 accuracy (21 samples)\n",
      "Categories with transparent glazes (ID: 135): 1.0000 accuracy (21 samples)\n",
      "Unglazed categories (literally: Categories without vitreous coating) (ID: 137): 0.9500 accuracy (20 samples)\n",
      "Categories in artificial pastes (ID: 140): 1.0000 accuracy (21 samples)\n",
      "Categories with opaque or opacified coating (ID: 144): 0.9524 accuracy (21 samples)\n",
      "\n",
      "==============================\n",
      "CONFUSION MATRIX:\n",
      "==============================\n",
      "Rows: True labels, Columns: Predicted labels\n",
      "                                                                      Categories with transparent glazes on slip  \\\n",
      "Categories with transparent glazes on slip                                                                    17   \n",
      "Categories with transparent glazes                                                                             0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                           0   \n",
      "Categories in artificial pastes                                                                                0   \n",
      "Categories with opaque or opacified coating                                                                    0   \n",
      "\n",
      "                                                                      Categories with transparent glazes  \\\n",
      "Categories with transparent glazes on slip                                                             3   \n",
      "Categories with transparent glazes                                                                    21   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                   1   \n",
      "Categories in artificial pastes                                                                        0   \n",
      "Categories with opaque or opacified coating                                                            1   \n",
      "\n",
      "                                                                      Unglazed categories (literally: Categories without vitreous coating)  \\\n",
      "Categories with transparent glazes on slip                                                                                               0   \n",
      "Categories with transparent glazes                                                                                                       0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                                                    19   \n",
      "Categories in artificial pastes                                                                                                          0   \n",
      "Categories with opaque or opacified coating                                                                                              0   \n",
      "\n",
      "                                                                      Categories in artificial pastes  \\\n",
      "Categories with transparent glazes on slip                                                          0   \n",
      "Categories with transparent glazes                                                                  0   \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                0   \n",
      "Categories in artificial pastes                                                                    21   \n",
      "Categories with opaque or opacified coating                                                         0   \n",
      "\n",
      "                                                                      Categories with opaque or opacified coating  \n",
      "Categories with transparent glazes on slip                                                                      1  \n",
      "Categories with transparent glazes                                                                              0  \n",
      "Unglazed categories (literally: Categories without vitreous coating)                                            0  \n",
      "Categories in artificial pastes                                                                                 0  \n",
      "Categories with opaque or opacified coating                                                                    20  \n",
      "Generating plots for baseline_architecture...\n",
      "  Saved MLP training history plot to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\\etude1_baseline_architecture_training_history.png\n",
      "Model and results saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\\baseline_architecture\n",
      "Test Accuracy for baseline_architecture: 0.9423\n",
      "\n",
      "--- MLP Training Test Complete ---\n",
      "Summary of results:\n",
      "  ✓ Scenario 'baseline_architecture' | Test Accuracy: 0.9423\n",
      "\n",
      "Models, scalers, and metadata saved in subdirectories of: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\mlp_classification_results\\etude1\n"
     ]
    }
   ],
   "source": [
    "# Import the main function for MLP classification\n",
    "from src.main_mlp_classification import run_mlp_classification_scenarios\n",
    "\n",
    "print(\"\\n--- Testing MLP Training and Evaluation Pipeline ---\")\n",
    "print(\"NOTE: This test uses the full data loaded in previous steps to train the model.\")\n",
    "\n",
    "STUDY_NAME_MLP = \"etude1\"\n",
    "# Type 0: Ceramic attributes only | Type 1: Functions/Features only | Type 2: Combined\n",
    "EMBEDDING_TYPE_MLP = 2\n",
    "\n",
    "print(f\"\\nRunning MLP classification for study: '{STUDY_NAME_MLP}' with embedding type: {EMBEDDING_TYPE_MLP}\")\n",
    "\n",
    "mlp_test_results = run_mlp_classification_scenarios(\n",
    "    dfs=dfs_raw,\n",
    "    study_name=STUDY_NAME_MLP,\n",
    "    embedding_type=EMBEDDING_TYPE_MLP\n",
    ")\n",
    "\n",
    "print(\"\\n--- MLP Training Test Complete ---\")\n",
    "if mlp_test_results:\n",
    "    print(\"Summary of results:\")\n",
    "    for scenario, accuracy in mlp_test_results.items():\n",
    "        if accuracy is not None:\n",
    "            print(f\"  ✓ Scenario '{scenario}' | Test Accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"  ✗ Scenario '{scenario}' failed.\")\n",
    "    output_dir = os.path.join(config.OUTPUT_BASE_DIR, 'mlp_classification_results', STUDY_NAME_MLP)\n",
    "    print(f\"\\nModels, scalers, and metadata saved in subdirectories of: {output_dir}\")\n",
    "else:\n",
    "    print(\"MLP training scenarios did not produce any results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RGCN+MlP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Full Data Preparation Pipeline (from main_prepare_data.py) ---\n",
      "This will generate all necessary files for both link prediction and classification tasks.\n",
      "This may take a few minutes, especially the BERT embedding generation...\n",
      "Using data path: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\data\n",
      "Seeded everything with seed 42\n",
      "Using data source path: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\data\n",
      "Loading data from: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\data\n",
      "  Loaded ceramic.csv as dfs['ceramic']\n",
      "  Loaded object_colors.csv as dfs['object_colors']\n",
      "  Loaded object_colors_attrib.csv as dfs['object_colors_attrib']\n",
      "  Loaded object_feature.csv as dfs['object_feature']\n",
      "  Loaded object_feature_combined_names.csv as dfs['object_feature_combined_names']\n",
      "  Loaded object_feature_attrib.csv as dfs['object_feature_attrib']\n",
      "  Loaded object_function_translated.csv as dfs['object_function']\n",
      "  Loaded object_function_attrib.csv as dfs['object_function_attrib']\n",
      "  Loaded tech_cat_translated.csv as dfs['tech_cat']\n",
      "  Loaded archaeological_sites.csv as dfs['archaeological_sites']\n",
      "  Loaded traditional_designation.csv as dfs['traditional_designation']\n",
      "  Loaded historical_period.csv as dfs['historical_period']\n",
      "  Loaded tech_cat_color_attrib.csv as dfs['tech_cat_color_attrib']\n",
      "  Loaded tech_cat_feature_attrib.csv as dfs['tech_cat_feature_attrib']\n",
      "  Loaded tech_cat_function_attrib.csv as dfs['tech_cat_function_attrib']\n",
      "  Loaded relations_type.csv as dfs['relations_type']\n",
      "  Loaded relations.csv as dfs['relations']\n",
      "  Loaded context_type_list.csv as dfs['context_type_list']\n",
      "  Loaded object_data_source.csv as dfs['object_data_source']\n",
      "  Loaded category_hierarchy_combined_names.csv as dfs['category_hierarchy_combined_names']\n",
      "  Loaded Features_Ontology_PF_translated.csv as dfs['Features_Ontology']\n",
      "Creating ceramic_summary DataFrame...\n",
      "  ceramic_summary created with 10482 rows.\n",
      "Ceramic Summary saved to: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\ceramic_summary_prepared.csv\n",
      "Generating one-hot embeddings...\n",
      "  OHE map sizes: origin=62, color=12, context=3, source=4\n",
      "  'Embedding' column added. Example length: 85\n",
      "Preparing Link Prediction Study Datasets...\n",
      "Identifying all valid ceramic IDs from ceramic_summary...\n",
      "Found 10482 unique valid ceramic IDs in summary.\n",
      "Building category hierarchy and mapping ceramics (for sampling info)...\n",
      "\n",
      "Building category hierarchy and mapping ceramics...\n",
      "Finding root for each category...\n",
      "Mapped 229 categories to their roots.\n",
      "Mapping ceramics to root categories...\n",
      "Mapped 8697 ceramics to a root category.\n",
      "\n",
      "📊 Ceramic Counts per Root Category:\n",
      "  - Root ID 140 ('Categories in artificial pastes'): 138 ceramics\n",
      "  - Root ID 135 ('Categories with transparent glazes'): 913 ceramics\n",
      "  - Root ID 144 ('Categories with opaque or opacified coating'): 949 ceramics\n",
      "  - Root ID 132 ('Categories with transparent glazes on slip'): 1396 ceramics\n",
      "  - Root ID 137 ('Unglazed categories (literally: Categories without vitreous coating)'): 5301 ceramics\n",
      "\n",
      "--- Target Sample Sizes ---\n",
      "Etude 1 (Min Count among roots: 138): Sample Size = 138\n",
      "Etude 1' (Fixed): Sample Size = 276\n",
      "Etude 2 (Exclude root 140, Min Rem Count: 913): Sample Size = 913\n",
      "Etude All: Using all 10482 ceramics from summary\n",
      "\n",
      "--- Preparing Dataset: etude1 ---\n",
      "  Sampling 138 ceramics per root from roots: {132, 135, 137, 140, 144}\n",
      "  Total unique ceramics selected for etude1: 690\n",
      "  Extracting triplets for 690 ceramics in etude1...\n",
      "Extracting triplets for 690 selected ceramics (handling lists)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\src\\graph_utils.py:242: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  new_col_values.append([pd.to_numeric(i, errors='ignore') for i in item])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extraction for selection. Got results structure for 690 ceramics.\n",
      "  Extracted 690 primary entries for triplet generation.\n",
      "  Formatting data for RGCN model for etude1...\n",
      "  Formatting etude1 data for RGCN with ALL-BERT Embeddings (BERT: paraphrase-multilingual-mpnet-base-v2)...\n",
      "    Using device: cpu\n",
      "    Loaded Sentence-BERT model 'paraphrase-multilingual-mpnet-base-v2' (Native Dim: 768)\n",
      "    Building category hierarchy to get root mapping (using original category IDs)...\n",
      "    Obtained mapping for 229 original categories to their original root IDs.\n",
      "    Identifying all unique nodes in the sampled data (Ceramics, Functions, Features, and ONLY ROOT Categories)...\n",
      "    Found 812 unique node identifiers in the sample (ceramics, functions, features, and ROOT categories).\n",
      "    Assigning graph indices and generating ALL-BERT embeddings...\n",
      "    ALL-BERT Embeddings - Generated: 812, Failed/Missing Text: 0\n",
      "\n",
      "    --- Node Type Examples with BERT Embeddings ---\n",
      "    Ceramic Node Example:\n",
      "      Node ID: Ceramic_10057\n",
      "      Text: 'A Ceramic ceramic from the consumption in Barralerie.'\n",
      "      Embedding (first 5 dims): [ 0.03479291 -0.24756464 -0.01379733  0.11515333 -0.03970487]\n",
      "\n",
      "    Function Node Example:\n",
      "      Node ID: Func_1\n",
      "      Text: 'dishware (or tableware)'\n",
      "      Embedding (first 5 dims): [ 0.0166124  -0.11477309 -0.00935472 -0.12030761 -0.0673581 ]\n",
      "\n",
      "    Category Node Example:\n",
      "      Node ID: Cat_132_Root_132\n",
      "      Text: 'Categories with transparent glazes on slip'\n",
      "      Embedding (first 5 dims): [ 0.04153645 -0.07711426 -0.01183737  0.10898539  0.06632965]\n",
      "\n",
      "    Feature Node Example:\n",
      "      Node ID: Feat_1\n",
      "      Text: 'Forming/Shaping'\n",
      "      Embedding (first 5 dims): [-0.15521666 -0.25642768 -0.01476311  0.05541804  0.01365412]\n",
      "\n",
      "    --- End Node Type Examples ---\n",
      "\n",
      "    Populated final ALL-BERT embedding matrix with shape (812 nodes, 768 dim). 812 nodes have non-zero embeddings.\n",
      "    Identifying root category node indices in the final graph map (all 'Cat_' nodes are roots)...\n",
      "    Identified 5 unique root category nodes in the graph's node_to_idx map.\n",
      "    Created cat_idx_to_root_idx_map with 5 entries (each root maps to itself).\n",
      "    Processing triplets using pre-assigned graph indices...\n",
      "    Added 0 RootCat->Function and 6 RootCat->Feature triplets (direct connections only).\n",
      "    Removed 6048 duplicate training triplets.\n",
      "    Removed 0 duplicate evaluation triplets (Ceramic->RootCat).\n",
      "\n",
      "    --- RGCN Data Summary (Hybrid Embeddings with PCA) ---\n",
      "    Study: etude1\n",
      "    BERT Model for Non-Ceramics: paraphrase-multilingual-mpnet-base-v2 (Native Dim: 768)\n",
      "    Ceramic Embeddings Source: OHE from 'Embedding' column (Expected Dim: 768)\n",
      "    Final Hybrid Embedding Dimension: 768\n",
      "    Total Nodes in Graph: 812\n",
      "    Nodes with Non-Zero Embedding in Final Matrix: 812\n",
      "    Total Relation Types in Graph: 11\n",
      "    Training Triplets (for GNN structure): 3605\n",
      "    Evaluation Triplets (Ceramic->RootCat, for BTC task): 690\n",
      "    Number of Root Category Nodes in Graph (for neg. sampling): 5\n",
      "    --- End Summary ---\n",
      "  ✅ Successfully prepared and formatted dataset for etude1.\n",
      "\n",
      "--- Preparing Dataset: etude1_prime ---\n",
      "  Sampling 276 ceramics per root from roots: {132, 135, 137, 140, 144}\n",
      "  Total unique ceramics selected for etude1_prime: 1242\n",
      "  Extracting triplets for 1242 ceramics in etude1_prime...\n",
      "Extracting triplets for 1242 selected ceramics (handling lists)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\src\\graph_utils.py:242: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  new_col_values.append([pd.to_numeric(i, errors='ignore') for i in item])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extraction for selection. Got results structure for 1242 ceramics.\n",
      "  Extracted 1242 primary entries for triplet generation.\n",
      "  Formatting data for RGCN model for etude1_prime...\n",
      "  Formatting etude1_prime data for RGCN with ALL-BERT Embeddings (BERT: paraphrase-multilingual-mpnet-base-v2)...\n",
      "    Using device: cpu\n",
      "    Loaded Sentence-BERT model 'paraphrase-multilingual-mpnet-base-v2' (Native Dim: 768)\n",
      "    Building category hierarchy to get root mapping (using original category IDs)...\n",
      "    Obtained mapping for 229 original categories to their original root IDs.\n",
      "    Identifying all unique nodes in the sampled data (Ceramics, Functions, Features, and ONLY ROOT Categories)...\n",
      "    Found 1404 unique node identifiers in the sample (ceramics, functions, features, and ROOT categories).\n",
      "    Assigning graph indices and generating ALL-BERT embeddings...\n",
      "    ALL-BERT Embeddings - Generated: 1404, Failed/Missing Text: 0\n",
      "\n",
      "    --- Node Type Examples with BERT Embeddings ---\n",
      "    Ceramic Node Example:\n",
      "      Node ID: Ceramic_10057\n",
      "      Text: 'A Ceramic ceramic from the consumption in Barralerie.'\n",
      "      Embedding (first 5 dims): [ 0.03479291 -0.24756464 -0.01379733  0.11515333 -0.03970487]\n",
      "\n",
      "    Function Node Example:\n",
      "      Node ID: Func_1\n",
      "      Text: 'dishware (or tableware)'\n",
      "      Embedding (first 5 dims): [ 0.0166124  -0.11477309 -0.00935472 -0.12030761 -0.0673581 ]\n",
      "\n",
      "    Category Node Example:\n",
      "      Node ID: Cat_132_Root_132\n",
      "      Text: 'Categories with transparent glazes on slip'\n",
      "      Embedding (first 5 dims): [ 0.04153645 -0.07711426 -0.01183737  0.10898539  0.06632965]\n",
      "\n",
      "    Feature Node Example:\n",
      "      Node ID: Feat_1\n",
      "      Text: 'Forming/Shaping'\n",
      "      Embedding (first 5 dims): [-0.15521666 -0.25642768 -0.01476311  0.05541804  0.01365412]\n",
      "\n",
      "    --- End Node Type Examples ---\n",
      "\n",
      "    Populated final ALL-BERT embedding matrix with shape (1404 nodes, 768 dim). 1404 nodes have non-zero embeddings.\n",
      "    Identifying root category node indices in the final graph map (all 'Cat_' nodes are roots)...\n",
      "    Identified 5 unique root category nodes in the graph's node_to_idx map.\n",
      "    Created cat_idx_to_root_idx_map with 5 entries (each root maps to itself).\n",
      "    Processing triplets using pre-assigned graph indices...\n",
      "    Added 0 RootCat->Function and 6 RootCat->Feature triplets (direct connections only).\n",
      "    Removed 11528 duplicate training triplets.\n",
      "    Removed 0 duplicate evaluation triplets (Ceramic->RootCat).\n",
      "\n",
      "    --- RGCN Data Summary (Hybrid Embeddings with PCA) ---\n",
      "    Study: etude1_prime\n",
      "    BERT Model for Non-Ceramics: paraphrase-multilingual-mpnet-base-v2 (Native Dim: 768)\n",
      "    Ceramic Embeddings Source: OHE from 'Embedding' column (Expected Dim: 768)\n",
      "    Final Hybrid Embedding Dimension: 768\n",
      "    Total Nodes in Graph: 1404\n",
      "    Nodes with Non-Zero Embedding in Final Matrix: 1404\n",
      "    Total Relation Types in Graph: 12\n",
      "    Training Triplets (for GNN structure): 6644\n",
      "    Evaluation Triplets (Ceramic->RootCat, for BTC task): 1242\n",
      "    Number of Root Category Nodes in Graph (for neg. sampling): 5\n",
      "    --- End Summary ---\n",
      "  ✅ Successfully prepared and formatted dataset for etude1_prime.\n",
      "\n",
      "--- Preparing Dataset: etude2 ---\n",
      "  Sampling 913 ceramics per root from roots: {132, 135, 137, 144}\n",
      "  Total unique ceramics selected for etude2: 3652\n",
      "  Extracting triplets for 3652 ceramics in etude2...\n",
      "Extracting triplets for 3652 selected ceramics (handling lists)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\src\\graph_utils.py:242: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  new_col_values.append([pd.to_numeric(i, errors='ignore') for i in item])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extraction for selection. Got results structure for 3652 ceramics.\n",
      "  Extracted 3652 primary entries for triplet generation.\n",
      "  Formatting data for RGCN model for etude2...\n",
      "  Formatting etude2 data for RGCN with ALL-BERT Embeddings (BERT: paraphrase-multilingual-mpnet-base-v2)...\n",
      "    Using device: cpu\n",
      "    Loaded Sentence-BERT model 'paraphrase-multilingual-mpnet-base-v2' (Native Dim: 768)\n",
      "    Building category hierarchy to get root mapping (using original category IDs)...\n",
      "    Obtained mapping for 229 original categories to their original root IDs.\n",
      "    Identifying all unique nodes in the sampled data (Ceramics, Functions, Features, and ONLY ROOT Categories)...\n",
      "    Found 3857 unique node identifiers in the sample (ceramics, functions, features, and ROOT categories).\n",
      "    Assigning graph indices and generating ALL-BERT embeddings...\n",
      "    ALL-BERT Embeddings - Generated: 3857, Failed/Missing Text: 0\n",
      "\n",
      "    --- Node Type Examples with BERT Embeddings ---\n",
      "    Ceramic Node Example:\n",
      "      Node ID: Ceramic_10000\n",
      "      Text: 'A Ceramic ceramic from the consumption in Barralerie.'\n",
      "      Embedding (first 5 dims): [ 0.03479291 -0.24756464 -0.01379733  0.11515333 -0.03970487]\n",
      "\n",
      "    Function Node Example:\n",
      "      Node ID: Func_1\n",
      "      Text: 'dishware (or tableware)'\n",
      "      Embedding (first 5 dims): [ 0.0166124  -0.11477309 -0.00935472 -0.12030761 -0.0673581 ]\n",
      "\n",
      "    Category Node Example:\n",
      "      Node ID: Cat_132_Root_132\n",
      "      Text: 'Categories with transparent glazes on slip'\n",
      "      Embedding (first 5 dims): [ 0.04153645 -0.07711426 -0.01183737  0.10898539  0.06632965]\n",
      "\n",
      "    Feature Node Example:\n",
      "      Node ID: Feat_1\n",
      "      Text: 'Forming/Shaping'\n",
      "      Embedding (first 5 dims): [-0.15521666 -0.25642768 -0.01476311  0.05541804  0.01365412]\n",
      "\n",
      "    --- End Node Type Examples ---\n",
      "\n",
      "    Populated final ALL-BERT embedding matrix with shape (3857 nodes, 768 dim). 3857 nodes have non-zero embeddings.\n",
      "    Identifying root category node indices in the final graph map (all 'Cat_' nodes are roots)...\n",
      "    Identified 5 unique root category nodes in the graph's node_to_idx map.\n",
      "    Created cat_idx_to_root_idx_map with 5 entries (each root maps to itself).\n",
      "    Processing triplets using pre-assigned graph indices...\n",
      "    Added 0 RootCat->Function and 3 RootCat->Feature triplets (direct connections only).\n",
      "    Removed 36344 duplicate training triplets.\n",
      "    Removed 0 duplicate evaluation triplets (Ceramic->RootCat).\n",
      "\n",
      "    --- RGCN Data Summary (Hybrid Embeddings with PCA) ---\n",
      "    Study: etude2\n",
      "    BERT Model for Non-Ceramics: paraphrase-multilingual-mpnet-base-v2 (Native Dim: 768)\n",
      "    Ceramic Embeddings Source: OHE from 'Embedding' column (Expected Dim: 768)\n",
      "    Final Hybrid Embedding Dimension: 768\n",
      "    Total Nodes in Graph: 3857\n",
      "    Nodes with Non-Zero Embedding in Final Matrix: 3857\n",
      "    Total Relation Types in Graph: 12\n",
      "    Training Triplets (for GNN structure): 19960\n",
      "    Evaluation Triplets (Ceramic->RootCat, for BTC task): 3652\n",
      "    Number of Root Category Nodes in Graph (for neg. sampling): 5\n",
      "    --- End Summary ---\n",
      "  ✅ Successfully prepared and formatted dataset for etude2.\n",
      "\n",
      "--- Finished Preparing All Study Datasets ---\n",
      "Saving link prediction study datasets...\n",
      "\n",
      "--- Saving Study Datasets to subdirectories under: 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets' ---\n",
      "\n",
      "--- Processing study: 'etude1' ---\n",
      "Saving 'etude1' data to directory: 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1'\n",
      "  Output directory 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1' ensured.\n",
      "    Node embeddings saved to 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1\\node_embeddings.npy' (Shape: (812, 768))\n",
      "  Finished saving for 'etude1'. 7 files created/attempted in 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1'.\n",
      "\n",
      "--- Processing study: 'etude1_prime' ---\n",
      "Saving 'etude1_prime' data to directory: 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1_prime'\n",
      "  Output directory 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1_prime' ensured.\n",
      "    Node embeddings saved to 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1_prime\\node_embeddings.npy' (Shape: (1404, 768))\n",
      "  Finished saving for 'etude1_prime'. 7 files created/attempted in 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude1_prime'.\n",
      "\n",
      "--- Processing study: 'etude2' ---\n",
      "Saving 'etude2' data to directory: 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude2'\n",
      "  Output directory 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude2' ensured.\n",
      "    Node embeddings saved to 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude2\\node_embeddings.npy' (Shape: (3857, 768))\n",
      "  Finished saving for 'etude2'. 7 files created/attempted in 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets\\etude2'.\n",
      "\n",
      "--- Finished saving all study datasets under 'c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\rgcn_study_datasets' ---\n",
      "Preparing Classification Study Datasets...\n",
      "\n",
      ">>> Processing and saving classification data for study: etude1 <<<\n",
      "\n",
      "--- Adapting RGCN Data for Ceramic Classification (Root Labels) ---\n",
      "  Original nodes: 812\n",
      "  Nodes kept for classification: 812\n",
      "  Extracting root category labels from 4295 available triplets...\n",
      "  Looking for relation ID 0 ('BELONGS_TO_CATEGORY')\n",
      "  Extracted labels for 690 ceramic nodes. Found 5 unique root labels.\n",
      "  Label mapping (5 labels):\n",
      "    Label 0: ' Category with transparent glazes' (Original Root ID: 135, 138 ceramics)\n",
      "    Label 1: ' Category in artificial pastes' (Original Root ID: 140, 138 ceramics)\n",
      "    Label 2: ' Category with transparent glazes on slip' (Original Root ID: 132, 138 ceramics)\n",
      "    Label 3: ' Category with opaque or opacified coating' (Original Root ID: 144, 138 ceramics)\n",
      "    Label 4: ' Category Unglazed categories (literally: Categories without vitreous coating)' (Original Root ID: 137, 138 ceramics)\n",
      "  Identified 690 ceramic nodes in the new mapping for classification.\n",
      "\n",
      "--- Adaptation for Classification Complete ---\n",
      "  Final ceramic nodes with labels: 690\n",
      "  Total classes: 5\n",
      "  Output directory created/ensured: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude1_root_classification_data\n",
      "  Saving data components...\n",
      "    Saved node_embeddings.npy\n",
      "    Saved node_mapping.csv\n",
      "    Saved relation_mapping.csv\n",
      "    Saved training_triplets.csv\n",
      "    Ceramic labels DataFrame shape: (690, 2)\n",
      "    Ceramic labels DataFrame dtypes:\n",
      "new_ceramic_node_idx    int64\n",
      "label_id                int64\n",
      "dtype: object\n",
      "    Ceramic labels DataFrame head:\n",
      "   new_ceramic_node_idx  label_id\n",
      "0                     5         0\n",
      "1                     6         0\n",
      "2                     7         0\n",
      "3                     8         0\n",
      "4                     9         1\n",
      "    Saved ceramic_labels.csv\n",
      "    Saved label_mapping.csv\n",
      "    Saved classification_stats.json\n",
      "  ✅ Successfully processed and saved classification data for 'etude1' to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude1_root_classification_data\n",
      "\n",
      ">>> Processing and saving classification data for study: etude1_prime <<<\n",
      "\n",
      "--- Adapting RGCN Data for Ceramic Classification (Root Labels) ---\n",
      "  Original nodes: 1404\n",
      "  Nodes kept for classification: 1404\n",
      "  Extracting root category labels from 7886 available triplets...\n",
      "  Looking for relation ID 0 ('BELONGS_TO_CATEGORY')\n",
      "  Extracted labels for 1242 ceramic nodes. Found 5 unique root labels.\n",
      "  Label mapping (5 labels):\n",
      "    Label 0: ' Category with transparent glazes' (Original Root ID: 135, 276 ceramics)\n",
      "    Label 1: ' Category with opaque or opacified coating' (Original Root ID: 144, 276 ceramics)\n",
      "    Label 2: ' Category in artificial pastes' (Original Root ID: 140, 138 ceramics)\n",
      "    Label 3: ' Category with transparent glazes on slip' (Original Root ID: 132, 276 ceramics)\n",
      "    Label 4: ' Category Unglazed categories (literally: Categories without vitreous coating)' (Original Root ID: 137, 276 ceramics)\n",
      "  Identified 1242 ceramic nodes in the new mapping for classification.\n",
      "\n",
      "--- Adaptation for Classification Complete ---\n",
      "  Final ceramic nodes with labels: 1242\n",
      "  Total classes: 5\n",
      "  Output directory created/ensured: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude1_prime_root_classification_data\n",
      "  Saving data components...\n",
      "    Saved node_embeddings.npy\n",
      "    Saved node_mapping.csv\n",
      "    Saved relation_mapping.csv\n",
      "    Saved training_triplets.csv\n",
      "    Ceramic labels DataFrame shape: (1242, 2)\n",
      "    Ceramic labels DataFrame dtypes:\n",
      "new_ceramic_node_idx    int64\n",
      "label_id                int64\n",
      "dtype: object\n",
      "    Ceramic labels DataFrame head:\n",
      "   new_ceramic_node_idx  label_id\n",
      "0                     5         0\n",
      "1                     6         0\n",
      "2                     7         0\n",
      "3                     8         0\n",
      "4                     9         1\n",
      "    Saved ceramic_labels.csv\n",
      "    Saved label_mapping.csv\n",
      "    Saved classification_stats.json\n",
      "  ✅ Successfully processed and saved classification data for 'etude1_prime' to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude1_prime_root_classification_data\n",
      "\n",
      ">>> Processing and saving classification data for study: etude2 <<<\n",
      "\n",
      "--- Adapting RGCN Data for Ceramic Classification (Root Labels) ---\n",
      "  Original nodes: 3857\n",
      "  Nodes kept for classification: 3857\n",
      "  Extracting root category labels from 23612 available triplets...\n",
      "  Looking for relation ID 0 ('BELONGS_TO_CATEGORY')\n",
      "  Extracted labels for 3652 ceramic nodes. Found 4 unique root labels.\n",
      "  Label mapping (4 labels):\n",
      "    Label 0: ' Category Unglazed categories (literally: Categories without vitreous coating)' (Original Root ID: 137, 913 ceramics)\n",
      "    Label 1: ' Category with transparent glazes' (Original Root ID: 135, 913 ceramics)\n",
      "    Label 2: ' Category with opaque or opacified coating' (Original Root ID: 144, 913 ceramics)\n",
      "    Label 3: ' Category with transparent glazes on slip' (Original Root ID: 132, 913 ceramics)\n",
      "  Identified 3652 ceramic nodes in the new mapping for classification.\n",
      "\n",
      "--- Adaptation for Classification Complete ---\n",
      "  Final ceramic nodes with labels: 3652\n",
      "  Total classes: 4\n",
      "  Output directory created/ensured: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude2_root_classification_data\n",
      "  Saving data components...\n",
      "    Saved node_embeddings.npy\n",
      "    Saved node_mapping.csv\n",
      "    Saved relation_mapping.csv\n",
      "    Saved training_triplets.csv\n",
      "    Ceramic labels DataFrame shape: (3652, 2)\n",
      "    Ceramic labels DataFrame dtypes:\n",
      "new_ceramic_node_idx    int64\n",
      "label_id                int64\n",
      "dtype: object\n",
      "    Ceramic labels DataFrame head:\n",
      "   new_ceramic_node_idx  label_id\n",
      "0                     5         0\n",
      "1                     6         0\n",
      "2                     7         0\n",
      "3                     8         0\n",
      "4                     9         0\n",
      "    Saved ceramic_labels.csv\n",
      "    Saved label_mapping.csv\n",
      "    Saved classification_stats.json\n",
      "  ✅ Successfully processed and saved classification data for 'etude2' to c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude2_root_classification_data\n",
      "Data Preparation Script Finished\n",
      "\n",
      "--- Full Data Preparation Complete ---\n",
      "The necessary files, including 'data.pt' for classification, should now be created in the 'output' directory.\n",
      "You can now re-run the 'RGCN+MLP Training and Evaluation' cell below.\n"
     ]
    }
   ],
   "source": [
    "from src.main_prepare_data import main as prepare_all_data\n",
    "\n",
    "print(\"\\n--- Running Full Data Preparation Pipeline (from main_prepare_data.py) ---\")\n",
    "print(\"This will generate all necessary files for both link prediction and classification tasks.\")\n",
    "print(\"This may take a few minutes, especially the BERT embedding generation...\")\n",
    "if 'LOCAL_DATA_PATH_TEST' in locals():\n",
    "    print(f\"Using data path: {LOCAL_DATA_PATH_TEST}\")\n",
    "    prepare_all_data(data_source_path=LOCAL_DATA_PATH_TEST)\n",
    "    \n",
    "    print(\"\\n--- Full Data Preparation Complete ---\")\n",
    "    print(\"The necessary files, including 'data.pt' for classification, should now be created in the 'output' directory.\")\n",
    "    print(\"You can now re-run the 'RGCN+MLP Training and Evaluation' cell below.\")\n",
    "else:\n",
    "    print(\"[ERROR] LOCAL_DATA_PATH_TEST variable not found. Cannot run data preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing RGCN+MLP Training and Evaluation Pipeline ---\n",
      "NOTE: This test requires the full pre-processed graph data from 'main_prepare_data.py'.\n",
      "It will fail if the data has not been generated first.\n",
      "\n",
      "Found classification data for study 'etude1'.\n",
      "Proceeding with a short training session (2 epochs)...\n",
      "\n",
      ">>> Training and Testing Classification Model for study: etude1 <<<\n",
      "Loading classification data from: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\etude1_root_classification_data\n",
      "\n",
      "Node type distribution:\n",
      "  Type 0 (Only outgoing): 692\n",
      "  Type 1 (Only incoming): 12\n",
      "  Type 2 (Bidirectional): 105\n",
      "  Type 3 (Isolated): 3\n",
      "Loaded data: 812 nodes, 11 relations, 3605 edges, 690 labeled nodes, 5 classes.\n",
      "Overriding hyperparameters:\n",
      "  rgcn_hidden_dim: 64 -> 32\n",
      "  learning_rate: 5e-05 -> 0.001\n",
      "  patience: 20 -> 20\n",
      "  dropout: 0.2 -> 0.2\n",
      "  l2_reg: 0.0001 -> 1e-05\n",
      "Creating data loaders for etude1\n",
      "Total labeled samples: 690\n",
      "Data split - Train: 482, Val: 104, Test: 104\n",
      "Instantiating RGCNClassifier model...\n",
      "Initializing RGCNClassifier:\n",
      "  - Nodes: 812\n",
      "  - Base relations: 11\n",
      "  - Total relations (with inverses): 22\n",
      "  - Embedding dim: 768\n",
      "  - Classes: 5\n",
      "  - Original edges: 3605\n",
      "  - Edge types: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCN Layer 0: 768 → 384 (relations: 22)\n",
      "RGCN Layer 1: 384 → 32 (relations: 22)\n",
      "Adding inverse relations...\n",
      "  - Original edges: 3605\n",
      "  - After adding inverses: 7210\n",
      "  - Original relation types: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "  - All relation types: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "Model instantiated successfully.\n",
      "PyTorch Lightning Trainer initialized.\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "   | Name                | Type                | Params | Mode \n",
      "---------------------------------------------------------------------\n",
      "0  | train_acc           | MulticlassAccuracy  | 0      | train\n",
      "1  | val_acc             | MulticlassAccuracy  | 0      | train\n",
      "2  | test_acc            | MulticlassAccuracy  | 0      | train\n",
      "3  | train_f1            | MulticlassF1Score   | 0      | train\n",
      "4  | val_f1              | MulticlassF1Score   | 0      | train\n",
      "5  | test_f1             | MulticlassF1Score   | 0      | train\n",
      "6  | test_precision      | MulticlassPrecision | 0      | train\n",
      "7  | test_recall         | MulticlassRecall    | 0      | train\n",
      "8  | node_emb            | Embedding           | 623 K  | train\n",
      "9  | rgcn_layers         | ModuleList          | 2.2 M  | train\n",
      "10 | dropout_layer       | Dropout             | 0      | train\n",
      "11 | classification_head | Sequential          | 677    | train\n",
      "12 | loss_fn             | CrossEntropyLoss    | 0      | train\n",
      "---------------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.101    Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AdamW optimizer with weight_decay=1e-05\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=ude1, val_loss=1.630, val_acc=0.260, val_f1=0.155, train_loss=2.490, train_acc=0.195, train_f1=0.095]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.632\n",
      "Epoch 0, global step 1: 'val_loss' reached 1.63206 (best 1.63206), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=0-val_loss=1.6321.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.80it/s, v_num=ude1, val_loss=1.500, val_acc=0.279, val_f1=0.181, train_loss=1.640, train_acc=0.210, train_f1=0.138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.131 >= min_delta = 0.0. New best score: 1.501\n",
      "Epoch 1, global step 2: 'val_loss' reached 1.50096 (best 1.50096), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=1-val_loss=1.5010.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=ude1, val_loss=1.390, val_acc=0.404, val_f1=0.313, train_loss=1.520, train_acc=0.274, train_f1=0.215]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.108 >= min_delta = 0.0. New best score: 1.393\n",
      "Epoch 2, global step 3: 'val_loss' reached 1.39318 (best 1.39318), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=2-val_loss=1.3932.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:01<00:00,  0.66it/s, v_num=ude1, val_loss=1.260, val_acc=0.529, val_f1=0.468, train_loss=1.380, train_acc=0.378, train_f1=0.324]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.137 >= min_delta = 0.0. New best score: 1.256\n",
      "Epoch 3, global step 4: 'val_loss' reached 1.25643 (best 1.25643), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=3-val_loss=1.2564.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:01<00:00,  0.68it/s, v_num=ude1, val_loss=1.140, val_acc=0.577, val_f1=0.517, train_loss=1.260, train_acc=0.469, train_f1=0.440]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.117 >= min_delta = 0.0. New best score: 1.139\n",
      "Epoch 4, global step 5: 'val_loss' reached 1.13894 (best 1.13894), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=4-val_loss=1.1389.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:01<00:00,  0.73it/s, v_num=ude1, val_loss=1.050, val_acc=0.635, val_f1=0.554, train_loss=1.160, train_acc=0.546, train_f1=0.516]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.084 >= min_delta = 0.0. New best score: 1.055\n",
      "Epoch 5, global step 6: 'val_loss' reached 1.05495 (best 1.05495), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=5-val_loss=1.0549.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [00:01<00:00,  0.72it/s, v_num=ude1, val_loss=0.972, val_acc=0.625, val_f1=0.536, train_loss=1.080, train_acc=0.577, train_f1=0.540]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.082 >= min_delta = 0.0. New best score: 0.972\n",
      "Epoch 6, global step 7: 'val_loss' reached 0.97246 (best 0.97246), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=6-val_loss=0.9725.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1/1 [00:01<00:00,  0.72it/s, v_num=ude1, val_loss=0.884, val_acc=0.644, val_f1=0.565, train_loss=1.070, train_acc=0.527, train_f1=0.464]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.088 >= min_delta = 0.0. New best score: 0.884\n",
      "Epoch 7, global step 8: 'val_loss' reached 0.88398 (best 0.88398), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=7-val_loss=0.8840.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=ude1, val_loss=0.813, val_acc=0.654, val_f1=0.584, train_loss=0.979, train_acc=0.539, train_f1=0.480]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.071 >= min_delta = 0.0. New best score: 0.813\n",
      "Epoch 8, global step 9: 'val_loss' reached 0.81260 (best 0.81260), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=8-val_loss=0.8126.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  0.67it/s, v_num=ude1, val_loss=0.757, val_acc=0.712, val_f1=0.644, train_loss=0.948, train_acc=0.568, train_f1=0.514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.056 >= min_delta = 0.0. New best score: 0.757\n",
      "Epoch 9, global step 10: 'val_loss' reached 0.75682 (best 0.75682), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=9-val_loss=0.7568.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=ude1, val_loss=0.711, val_acc=0.702, val_f1=0.635, train_loss=0.838, train_acc=0.639, train_f1=0.588]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.046 >= min_delta = 0.0. New best score: 0.711\n",
      "Epoch 10, global step 11: 'val_loss' reached 0.71102 (best 0.71102), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=10-val_loss=0.7110.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1/1 [00:01<00:00,  0.63it/s, v_num=ude1, val_loss=0.679, val_acc=0.692, val_f1=0.627, train_loss=0.783, train_acc=0.647, train_f1=0.593]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.679\n",
      "Epoch 11, global step 12: 'val_loss' reached 0.67937 (best 0.67937), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=11-val_loss=0.6794.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1/1 [00:01<00:00,  0.70it/s, v_num=ude1, val_loss=0.655, val_acc=0.712, val_f1=0.645, train_loss=0.793, train_acc=0.651, train_f1=0.601]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.655\n",
      "Epoch 12, global step 13: 'val_loss' reached 0.65489 (best 0.65489), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=12-val_loss=0.6549.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1/1 [00:01<00:00,  0.67it/s, v_num=ude1, val_loss=0.634, val_acc=0.731, val_f1=0.663, train_loss=0.728, train_acc=0.674, train_f1=0.632]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.634\n",
      "Epoch 13, global step 14: 'val_loss' reached 0.63437 (best 0.63437), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=13-val_loss=0.6344.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1/1 [00:01<00:00,  0.68it/s, v_num=ude1, val_loss=0.614, val_acc=0.750, val_f1=0.702, train_loss=0.739, train_acc=0.683, train_f1=0.640]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.614\n",
      "Epoch 14, global step 15: 'val_loss' reached 0.61396 (best 0.61396), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=14-val_loss=0.6140.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1/1 [00:01<00:00,  0.68it/s, v_num=ude1, val_loss=0.582, val_acc=0.779, val_f1=0.756, train_loss=0.663, train_acc=0.703, train_f1=0.654]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.582\n",
      "Epoch 15, global step 16: 'val_loss' reached 0.58187 (best 0.58187), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=15-val_loss=0.5819.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1/1 [00:01<00:00,  0.59it/s, v_num=ude1, val_loss=0.551, val_acc=0.769, val_f1=0.726, train_loss=0.634, train_acc=0.728, train_f1=0.680]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.551\n",
      "Epoch 16, global step 17: 'val_loss' reached 0.55132 (best 0.55132), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=16-val_loss=0.5513.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1/1 [00:01<00:00,  0.59it/s, v_num=ude1, val_loss=0.524, val_acc=0.769, val_f1=0.726, train_loss=0.639, train_acc=0.689, train_f1=0.649]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.028 >= min_delta = 0.0. New best score: 0.524\n",
      "Epoch 17, global step 18: 'val_loss' reached 0.52363 (best 0.52363), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=17-val_loss=0.5236.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1/1 [00:01<00:00,  0.59it/s, v_num=ude1, val_loss=0.498, val_acc=0.750, val_f1=0.681, train_loss=0.594, train_acc=0.724, train_f1=0.668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.026 >= min_delta = 0.0. New best score: 0.498\n",
      "Epoch 18, global step 19: 'val_loss' reached 0.49767 (best 0.49767), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=18-val_loss=0.4977.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1/1 [00:01<00:00,  0.65it/s, v_num=ude1, val_loss=0.474, val_acc=0.760, val_f1=0.695, train_loss=0.597, train_acc=0.724, train_f1=0.659]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.474\n",
      "Epoch 19, global step 20: 'val_loss' reached 0.47420 (best 0.47420), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=19-val_loss=0.4742.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=ude1, val_loss=0.458, val_acc=0.760, val_f1=0.695, train_loss=0.571, train_acc=0.722, train_f1=0.661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.458\n",
      "Epoch 20, global step 21: 'val_loss' reached 0.45847 (best 0.45847), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=20-val_loss=0.4585.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 1/1 [00:01<00:00,  0.58it/s, v_num=ude1, val_loss=0.445, val_acc=0.760, val_f1=0.694, train_loss=0.565, train_acc=0.739, train_f1=0.688]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.445\n",
      "Epoch 21, global step 22: 'val_loss' reached 0.44470 (best 0.44470), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=21-val_loss=0.4447.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 1/1 [00:01<00:00,  0.58it/s, v_num=ude1, val_loss=0.435, val_acc=0.760, val_f1=0.695, train_loss=0.555, train_acc=0.728, train_f1=0.668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.435\n",
      "Epoch 22, global step 23: 'val_loss' reached 0.43522 (best 0.43522), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=22-val_loss=0.4352.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 1/1 [00:01<00:00,  0.58it/s, v_num=ude1, val_loss=0.423, val_acc=0.760, val_f1=0.695, train_loss=0.543, train_acc=0.753, train_f1=0.709]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.423\n",
      "Epoch 23, global step 24: 'val_loss' reached 0.42260 (best 0.42260), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=23-val_loss=0.4226.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 1/1 [00:02<00:00,  0.49it/s, v_num=ude1, val_loss=0.413, val_acc=0.769, val_f1=0.718, train_loss=0.533, train_acc=0.753, train_f1=0.699]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.413\n",
      "Epoch 24, global step 25: 'val_loss' reached 0.41278 (best 0.41278), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=24-val_loss=0.4128.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 1/1 [00:01<00:00,  0.58it/s, v_num=ude1, val_loss=0.400, val_acc=0.769, val_f1=0.718, train_loss=0.494, train_acc=0.763, train_f1=0.709]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.400\n",
      "Epoch 25, global step 26: 'val_loss' reached 0.39995 (best 0.39995), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=25-val_loss=0.3999.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s, v_num=ude1, val_loss=0.391, val_acc=0.788, val_f1=0.754, train_loss=0.497, train_acc=0.761, train_f1=0.707]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.391\n",
      "Epoch 26, global step 27: 'val_loss' reached 0.39066 (best 0.39066), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=26-val_loss=0.3907.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.383, val_acc=0.817, val_f1=0.800, train_loss=0.461, train_acc=0.751, train_f1=0.702]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.383\n",
      "Epoch 27, global step 28: 'val_loss' reached 0.38323 (best 0.38323), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=27-val_loss=0.3832.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 1/1 [00:01<00:00,  0.58it/s, v_num=ude1, val_loss=0.376, val_acc=0.817, val_f1=0.800, train_loss=0.465, train_acc=0.759, train_f1=0.708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.376\n",
      "Epoch 28, global step 29: 'val_loss' reached 0.37619 (best 0.37619), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=28-val_loss=0.3762.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.373, val_acc=0.798, val_f1=0.766, train_loss=0.451, train_acc=0.761, train_f1=0.706]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.373\n",
      "Epoch 29, global step 30: 'val_loss' reached 0.37282 (best 0.37282), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=29-val_loss=0.3728.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.361, val_acc=0.798, val_f1=0.766, train_loss=0.425, train_acc=0.782, train_f1=0.729]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.361\n",
      "Epoch 30, global step 31: 'val_loss' reached 0.36128 (best 0.36128), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=30-val_loss=0.3613.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.346, val_acc=0.808, val_f1=0.776, train_loss=0.436, train_acc=0.770, train_f1=0.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.0. New best score: 0.346\n",
      "Epoch 31, global step 32: 'val_loss' reached 0.34569 (best 0.34569), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=31-val_loss=0.3457.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=ude1, val_loss=0.331, val_acc=0.817, val_f1=0.784, train_loss=0.432, train_acc=0.761, train_f1=0.707]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.331\n",
      "Epoch 32, global step 33: 'val_loss' reached 0.33134 (best 0.33134), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=32-val_loss=0.3313.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.324, val_acc=0.817, val_f1=0.784, train_loss=0.421, train_acc=0.788, train_f1=0.739]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.324\n",
      "Epoch 33, global step 34: 'val_loss' reached 0.32362 (best 0.32362), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=33-val_loss=0.3236.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.322, val_acc=0.837, val_f1=0.817, train_loss=0.437, train_acc=0.766, train_f1=0.715]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.322\n",
      "Epoch 34, global step 35: 'val_loss' reached 0.32169 (best 0.32169), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=34-val_loss=0.3217.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=ude1, val_loss=0.326, val_acc=0.837, val_f1=0.817, train_loss=0.407, train_acc=0.770, train_f1=0.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 36: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 1/1 [00:02<00:00,  0.44it/s, v_num=ude1, val_loss=0.329, val_acc=0.837, val_f1=0.817, train_loss=0.379, train_acc=0.805, train_f1=0.772]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 37: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 1/1 [00:01<00:00,  0.56it/s, v_num=ude1, val_loss=0.331, val_acc=0.837, val_f1=0.817, train_loss=0.397, train_acc=0.790, train_f1=0.746]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 38: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=ude1, val_loss=0.326, val_acc=0.846, val_f1=0.831, train_loss=0.396, train_acc=0.797, train_f1=0.768]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 39: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 1/1 [00:02<00:00,  0.50it/s, v_num=ude1, val_loss=0.314, val_acc=0.846, val_f1=0.831, train_loss=0.373, train_acc=0.820, train_f1=0.799]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.314\n",
      "Epoch 39, global step 40: 'val_loss' reached 0.31381 (best 0.31381), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=39-val_loss=0.3138.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s, v_num=ude1, val_loss=0.291, val_acc=0.846, val_f1=0.831, train_loss=0.362, train_acc=0.811, train_f1=0.785]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.291\n",
      "Epoch 40, global step 41: 'val_loss' reached 0.29100 (best 0.29100), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=40-val_loss=0.2910.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=ude1, val_loss=0.273, val_acc=0.856, val_f1=0.844, train_loss=0.343, train_acc=0.824, train_f1=0.799]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.273\n",
      "Epoch 41, global step 42: 'val_loss' reached 0.27257 (best 0.27257), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=41-val_loss=0.2726.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 1/1 [00:01<00:00,  0.50it/s, v_num=ude1, val_loss=0.264, val_acc=0.875, val_f1=0.866, train_loss=0.326, train_acc=0.813, train_f1=0.779]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.264\n",
      "Epoch 42, global step 43: 'val_loss' reached 0.26367 (best 0.26367), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=42-val_loss=0.2637.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 1/1 [00:01<00:00,  0.60it/s, v_num=ude1, val_loss=0.260, val_acc=0.875, val_f1=0.866, train_loss=0.321, train_acc=0.836, train_f1=0.815]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.260\n",
      "Epoch 43, global step 44: 'val_loss' reached 0.26031 (best 0.26031), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=43-val_loss=0.2603.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.258, val_acc=0.942, val_f1=0.942, train_loss=0.321, train_acc=0.832, train_f1=0.813]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.258\n",
      "Epoch 44, global step 45: 'val_loss' reached 0.25797 (best 0.25797), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=44-val_loss=0.2580.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.257, val_acc=0.971, val_f1=0.971, train_loss=0.311, train_acc=0.876, train_f1=0.867]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.257\n",
      "Epoch 45, global step 46: 'val_loss' reached 0.25654 (best 0.25654), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=45-val_loss=0.2565.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=ude1, val_loss=0.254, val_acc=0.981, val_f1=0.981, train_loss=0.295, train_acc=0.925, train_f1=0.925]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.254\n",
      "Epoch 46, global step 47: 'val_loss' reached 0.25445 (best 0.25445), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=46-val_loss=0.2545.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.246, val_acc=0.981, val_f1=0.981, train_loss=0.275, train_acc=0.917, train_f1=0.915]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.246\n",
      "Epoch 47, global step 48: 'val_loss' reached 0.24632 (best 0.24632), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=47-val_loss=0.2463.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 1/1 [00:02<00:00,  0.46it/s, v_num=ude1, val_loss=0.234, val_acc=0.981, val_f1=0.981, train_loss=0.264, train_acc=0.938, train_f1=0.937]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.234\n",
      "Epoch 48, global step 49: 'val_loss' reached 0.23449 (best 0.23449), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=48-val_loss=0.2345.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=ude1, val_loss=0.223, val_acc=0.981, val_f1=0.981, train_loss=0.276, train_acc=0.917, train_f1=0.915]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.223\n",
      "Epoch 49, global step 50: 'val_loss' reached 0.22257 (best 0.22257), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=49-val_loss=0.2226.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 1/1 [00:02<00:00,  0.44it/s, v_num=ude1, val_loss=0.210, val_acc=0.981, val_f1=0.981, train_loss=0.254, train_acc=0.936, train_f1=0.934]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.210\n",
      "Epoch 50, global step 51: 'val_loss' reached 0.20966 (best 0.20966), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=50-val_loss=0.2097.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 1/1 [00:02<00:00,  0.45it/s, v_num=ude1, val_loss=0.195, val_acc=0.981, val_f1=0.981, train_loss=0.236, train_acc=0.929, train_f1=0.927]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.195\n",
      "Epoch 51, global step 52: 'val_loss' reached 0.19529 (best 0.19529), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=51-val_loss=0.1953.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 1/1 [00:02<00:00,  0.39it/s, v_num=ude1, val_loss=0.191, val_acc=0.981, val_f1=0.981, train_loss=0.225, train_acc=0.919, train_f1=0.916]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.191\n",
      "Epoch 52, global step 53: 'val_loss' reached 0.19058 (best 0.19058), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=52-val_loss=0.1906.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=ude1, val_loss=0.191, val_acc=0.981, val_f1=0.981, train_loss=0.247, train_acc=0.911, train_f1=0.907]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 54: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 1/1 [00:02<00:00,  0.34it/s, v_num=ude1, val_loss=0.185, val_acc=0.981, val_f1=0.981, train_loss=0.218, train_acc=0.934, train_f1=0.933]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.185\n",
      "Epoch 54, global step 55: 'val_loss' reached 0.18497 (best 0.18497), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=54-val_loss=0.1850.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 1/1 [00:02<00:00,  0.38it/s, v_num=ude1, val_loss=0.170, val_acc=0.981, val_f1=0.981, train_loss=0.230, train_acc=0.913, train_f1=0.911]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.170\n",
      "Epoch 55, global step 56: 'val_loss' reached 0.16983 (best 0.16983), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=55-val_loss=0.1698.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 1/1 [00:02<00:00,  0.46it/s, v_num=ude1, val_loss=0.158, val_acc=0.981, val_f1=0.981, train_loss=0.205, train_acc=0.929, train_f1=0.928]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.158\n",
      "Epoch 56, global step 57: 'val_loss' reached 0.15788 (best 0.15788), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=56-val_loss=0.1579.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.162, val_acc=0.990, val_f1=0.990, train_loss=0.204, train_acc=0.927, train_f1=0.925]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 58: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 1/1 [00:01<00:00,  0.50it/s, v_num=ude1, val_loss=0.168, val_acc=0.981, val_f1=0.981, train_loss=0.199, train_acc=0.921, train_f1=0.918]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 59: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.167, val_acc=0.981, val_f1=0.981, train_loss=0.181, train_acc=0.950, train_f1=0.950]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59, global step 60: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=ude1, val_loss=0.163, val_acc=0.981, val_f1=0.981, train_loss=0.170, train_acc=0.950, train_f1=0.949]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 61: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 1/1 [00:02<00:00,  0.39it/s, v_num=ude1, val_loss=0.164, val_acc=0.981, val_f1=0.981, train_loss=0.183, train_acc=0.952, train_f1=0.951]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 62: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.169, val_acc=0.981, val_f1=0.981, train_loss=0.157, train_acc=0.952, train_f1=0.952]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 63: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 1/1 [00:02<00:00,  0.49it/s, v_num=ude1, val_loss=0.166, val_acc=0.981, val_f1=0.981, train_loss=0.185, train_acc=0.946, train_f1=0.945]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 64: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.151, val_acc=0.981, val_f1=0.981, train_loss=0.141, train_acc=0.985, train_f1=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.151\n",
      "Epoch 64, global step 65: 'val_loss' reached 0.15140 (best 0.15140), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=64-val_loss=0.1514.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 1/1 [00:01<00:00,  0.50it/s, v_num=ude1, val_loss=0.138, val_acc=0.981, val_f1=0.981, train_loss=0.154, train_acc=0.969, train_f1=0.969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.138\n",
      "Epoch 65, global step 66: 'val_loss' reached 0.13765 (best 0.13765), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=65-val_loss=0.1376.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 1/1 [00:02<00:00,  0.47it/s, v_num=ude1, val_loss=0.132, val_acc=0.981, val_f1=0.981, train_loss=0.143, train_acc=0.969, train_f1=0.969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.132\n",
      "Epoch 66, global step 67: 'val_loss' reached 0.13168 (best 0.13168), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=66-val_loss=0.1317.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.129, val_acc=0.981, val_f1=0.981, train_loss=0.148, train_acc=0.979, train_f1=0.979]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.129\n",
      "Epoch 67, global step 68: 'val_loss' reached 0.12906 (best 0.12906), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=67-val_loss=0.1291.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=ude1, val_loss=0.128, val_acc=0.981, val_f1=0.981, train_loss=0.140, train_acc=0.967, train_f1=0.966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.128\n",
      "Epoch 68, global step 69: 'val_loss' reached 0.12831 (best 0.12831), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=68-val_loss=0.1283.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 1/1 [00:02<00:00,  0.35it/s, v_num=ude1, val_loss=0.130, val_acc=0.981, val_f1=0.981, train_loss=0.146, train_acc=0.979, train_f1=0.979]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69, global step 70: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 1/1 [00:03<00:00,  0.33it/s, v_num=ude1, val_loss=0.133, val_acc=0.981, val_f1=0.981, train_loss=0.129, train_acc=0.975, train_f1=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70, global step 71: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 1/1 [00:02<00:00,  0.45it/s, v_num=ude1, val_loss=0.136, val_acc=0.981, val_f1=0.981, train_loss=0.128, train_acc=0.981, train_f1=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71, global step 72: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 1/1 [00:03<00:00,  0.28it/s, v_num=ude1, val_loss=0.136, val_acc=0.981, val_f1=0.981, train_loss=0.102, train_acc=0.985, train_f1=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72, global step 73: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 1/1 [00:04<00:00,  0.24it/s, v_num=ude1, val_loss=0.131, val_acc=0.981, val_f1=0.981, train_loss=0.103, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73, global step 74: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 1/1 [00:04<00:00,  0.24it/s, v_num=ude1, val_loss=0.119, val_acc=0.981, val_f1=0.981, train_loss=0.109, train_acc=0.983, train_f1=0.983]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.119\n",
      "Epoch 74, global step 75: 'val_loss' reached 0.11890 (best 0.11890), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=74-val_loss=0.1189.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 1/1 [00:02<00:00,  0.33it/s, v_num=ude1, val_loss=0.104, val_acc=0.981, val_f1=0.981, train_loss=0.109, train_acc=0.981, train_f1=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.104\n",
      "Epoch 75, global step 76: 'val_loss' reached 0.10421 (best 0.10421), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=75-val_loss=0.1042.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 1/1 [00:03<00:00,  0.32it/s, v_num=ude1, val_loss=0.0962, val_acc=0.981, val_f1=0.981, train_loss=0.111, train_acc=0.981, train_f1=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.096\n",
      "Epoch 76, global step 77: 'val_loss' reached 0.09616 (best 0.09616), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=76-val_loss=0.0962.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 1/1 [00:02<00:00,  0.34it/s, v_num=ude1, val_loss=0.0918, val_acc=0.981, val_f1=0.981, train_loss=0.0896, train_acc=0.985, train_f1=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.092\n",
      "Epoch 77, global step 78: 'val_loss' reached 0.09181 (best 0.09181), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=77-val_loss=0.0918.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 1/1 [00:02<00:00,  0.37it/s, v_num=ude1, val_loss=0.0878, val_acc=0.981, val_f1=0.981, train_loss=0.0876, train_acc=0.988, train_f1=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.088\n",
      "Epoch 78, global step 79: 'val_loss' reached 0.08784 (best 0.08784), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=78-val_loss=0.0878.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 1/1 [00:02<00:00,  0.36it/s, v_num=ude1, val_loss=0.0877, val_acc=0.981, val_f1=0.981, train_loss=0.0635, train_acc=0.998, train_f1=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.088\n",
      "Epoch 79, global step 80: 'val_loss' reached 0.08766 (best 0.08766), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=79-val_loss=0.0877.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 1/1 [00:01<00:00,  0.52it/s, v_num=ude1, val_loss=0.0855, val_acc=0.981, val_f1=0.981, train_loss=0.0869, train_acc=0.988, train_f1=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.085\n",
      "Epoch 80, global step 81: 'val_loss' reached 0.08546 (best 0.08546), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=80-val_loss=0.0855.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 1/1 [00:01<00:00,  0.56it/s, v_num=ude1, val_loss=0.0841, val_acc=0.981, val_f1=0.981, train_loss=0.0836, train_acc=0.985, train_f1=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.084\n",
      "Epoch 81, global step 82: 'val_loss' reached 0.08414 (best 0.08414), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=81-val_loss=0.0841.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 1/1 [00:02<00:00,  0.46it/s, v_num=ude1, val_loss=0.0836, val_acc=0.981, val_f1=0.981, train_loss=0.0761, train_acc=0.992, train_f1=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.084\n",
      "Epoch 82, global step 83: 'val_loss' reached 0.08363 (best 0.08363), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=82-val_loss=0.0836.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.086, val_acc=0.981, val_f1=0.981, train_loss=0.0731, train_acc=0.992, train_f1=0.992] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83, global step 84: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s, v_num=ude1, val_loss=0.0893, val_acc=0.981, val_f1=0.981, train_loss=0.0722, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84, global step 85: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 1/1 [00:02<00:00,  0.40it/s, v_num=ude1, val_loss=0.0923, val_acc=0.981, val_f1=0.981, train_loss=0.0709, train_acc=0.990, train_f1=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85, global step 86: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s, v_num=ude1, val_loss=0.0932, val_acc=0.981, val_f1=0.981, train_loss=0.0647, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86, global step 87: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.0889, val_acc=0.981, val_f1=0.981, train_loss=0.0716, train_acc=0.990, train_f1=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87, global step 88: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=ude1, val_loss=0.0836, val_acc=0.981, val_f1=0.981, train_loss=0.0675, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.084\n",
      "Epoch 88, global step 89: 'val_loss' reached 0.08359 (best 0.08359), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=88-val_loss=0.0836.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=ude1, val_loss=0.078, val_acc=0.981, val_f1=0.981, train_loss=0.0695, train_acc=0.988, train_f1=0.988] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.078\n",
      "Epoch 89, global step 90: 'val_loss' reached 0.07800 (best 0.07800), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=89-val_loss=0.0780.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 1/1 [00:01<00:00,  0.55it/s, v_num=ude1, val_loss=0.0741, val_acc=0.981, val_f1=0.981, train_loss=0.0741, train_acc=0.988, train_f1=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.074\n",
      "Epoch 90, global step 91: 'val_loss' reached 0.07410 (best 0.07410), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=90-val_loss=0.0741.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 1/1 [00:01<00:00,  0.57it/s, v_num=ude1, val_loss=0.0713, val_acc=0.981, val_f1=0.981, train_loss=0.0544, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.071\n",
      "Epoch 91, global step 92: 'val_loss' reached 0.07130 (best 0.07130), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=91-val_loss=0.0713.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 1/1 [00:02<00:00,  0.48it/s, v_num=ude1, val_loss=0.0703, val_acc=0.981, val_f1=0.981, train_loss=0.0583, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.070\n",
      "Epoch 92, global step 93: 'val_loss' reached 0.07026 (best 0.07026), saving model to 'C:\\\\Users\\\\moham\\\\OneDrive\\\\Desktop\\\\spiridon\\\\Spiridon\\\\output\\\\classification_data\\\\lightning_logs\\\\etude1\\\\checkpoints\\\\epoch=92-val_loss=0.0703.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 1/1 [00:01<00:00,  0.53it/s, v_num=ude1, val_loss=0.0744, val_acc=0.981, val_f1=0.981, train_loss=0.0684, train_acc=0.996, train_f1=0.996]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93, global step 94: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 1/1 [00:01<00:00,  0.56it/s, v_num=ude1, val_loss=0.080, val_acc=0.981, val_f1=0.981, train_loss=0.0586, train_acc=0.990, train_f1=0.990] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94, global step 95: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 1/1 [00:01<00:00,  0.51it/s, v_num=ude1, val_loss=0.0842, val_acc=0.981, val_f1=0.981, train_loss=0.0727, train_acc=0.988, train_f1=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95, global step 96: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 1/1 [00:01<00:00,  0.56it/s, v_num=ude1, val_loss=0.0876, val_acc=0.981, val_f1=0.981, train_loss=0.0626, train_acc=0.988, train_f1=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96, global step 97: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 1/1 [00:01<00:00,  0.54it/s, v_num=ude1, val_loss=0.0881, val_acc=0.981, val_f1=0.981, train_loss=0.044, train_acc=0.996, train_f1=0.996] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97, global step 98: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 1/1 [00:02<00:00,  0.50it/s, v_num=ude1, val_loss=0.0802, val_acc=0.981, val_f1=0.981, train_loss=0.0522, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98, global step 99: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:02<00:00,  0.41it/s, v_num=ude1, val_loss=0.0723, val_acc=0.981, val_f1=0.981, train_loss=0.0495, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99, global step 100: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 1/1 [00:02<00:00,  0.40it/s, v_num=ude1, val_loss=0.0723, val_acc=0.981, val_f1=0.981, train_loss=0.0495, train_acc=0.994, train_f1=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1\\checkpoints\\epoch=92-val_loss=0.0703.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed.\n",
      "Best model checkpoint: C:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1\\checkpoints\\epoch=92-val_loss=0.0703.ckpt\n",
      "Starting model testing...\n",
      "Loading best model from checkpoint for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at C:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1\\checkpoints\\epoch=92-val_loss=0.0703.ckpt\n",
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9903846383094788\n",
      "         test_f1            0.9904707670211792\n",
      "        test_loss           0.06511581689119339\n",
      "     test_precision         0.9909090399742126\n",
      "       test_recall          0.9904761910438538\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Final Test Metrics:\n",
      "  test_loss: 0.0651\n",
      "  test_acc: 0.9904\n",
      "  test_f1: 0.9905\n",
      "  test_precision: 0.9909\n",
      "  test_recall: 0.9905\n",
      "Testing completed.\n",
      "Plotting training history...\n",
      "  Metrics file not found after checking standard paths under c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1. Skipping plot.\n",
      "\n",
      ">>> Finished processing study: etude1 <<<\n",
      "\n",
      "--- RGCN+MLP Training Test Complete ---\n",
      "Test run finished successfully.\n",
      "  Log Directory: c:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1\n",
      "  Best Checkpoint: C:\\Users\\moham\\OneDrive\\Desktop\\spiridon\\Spiridon\\output\\classification_data\\lightning_logs\\etude1\\checkpoints\\epoch=92-val_loss=0.0703.ckpt\n",
      "  Test Metrics:\n",
      "    test_loss: 0.0651\n",
      "    test_acc: 0.9904\n",
      "    test_f1: 0.9905\n",
      "    test_precision: 0.9909\n",
      "    test_recall: 0.9905\n"
     ]
    }
   ],
   "source": [
    "# Import the main training function for the RGCN-based classifier\n",
    "from src.training.rgcn_mlp_classification_pipeline import train_and_test_classification_model\n",
    "import torch\n",
    "\n",
    "print(\"\\n--- Testing RGCN+MLP Training and Evaluation Pipeline ---\")\n",
    "print(\"NOTE: This test requires the full pre-processed graph data from 'main_prepare_data.py'.\")\n",
    "print(\"It will fail if the data has not been generated first.\")\n",
    "\n",
    "STUDY_NAME_RGCN = \"etude1\"\n",
    "clf_data_dir = os.path.join(config.CLASSIFICATION_DATA_DIR, f\"{STUDY_NAME_RGCN}_root_classification_data\")\n",
    "clf_data_file = os.path.join(clf_data_dir, \"data.pt\")\n",
    "\n",
    "\n",
    "print(f\"\\nFound classification data for study '{STUDY_NAME_RGCN}'.\")\n",
    "print(\"Proceeding with a short training session (2 epochs)...\")\n",
    "\n",
    "# Define lightweight hyperparameters and trainer params for a quick test\n",
    "test_hyperparams = {\n",
    "    'rgcn_hidden_dim': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'patience': 20,\n",
    "    'dropout': 0.2,\n",
    "    'l2_reg': 1e-5\n",
    "}\n",
    "\n",
    "test_trainer_params = {\n",
    "    'max_epochs': 100, # Just to test the pipeline runs\n",
    "    'accelerator': 'cpu', # Force CPU for simple test\n",
    "    'devices': 1,\n",
    "    'enable_progress_bar': True\n",
    "}\n",
    "\n",
    "# Run the training and evaluation pipeline\n",
    "rgcn_mlp_results = train_and_test_classification_model(\n",
    "    study_name=STUDY_NAME_RGCN,\n",
    "    base_data_dir=config.CLASSIFICATION_DATA_DIR,\n",
    "    hyperparameters=test_hyperparams,\n",
    "    trainer_params=test_trainer_params,\n",
    "    plot_history=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- RGCN+MLP Training Test Complete ---\")\n",
    "if rgcn_mlp_results:\n",
    "    print(\"Test run finished successfully.\")\n",
    "    print(f\"  Log Directory: {rgcn_mlp_results.get('log_dir', 'N/A')}\")\n",
    "    print(f\"  Best Checkpoint: {rgcn_mlp_results.get('best_checkpoint_path', 'N/A')}\")\n",
    "    print(\"  Test Metrics:\")\n",
    "    if rgcn_mlp_results.get('test_metrics'):\n",
    "        for metric, value in rgcn_mlp_results.get('test_metrics', {}).items():\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"RGCN+MLP training pipeline failed or was skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
